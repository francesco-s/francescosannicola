<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Francesco Sannicola Portfolio: software engineering and artificial intelligence."
    />
    <meta
      name="keywords"
      content="portfolio, machine learning, software, cloud, amazon web service, engineering"
    />
    <meta name="author" content="Francesco Sannicola" />
    <meta
      property="og:title"
      content="Francesco Sannicola - Machine Learning Engineer"
    />
    <meta
      property="og:description"
      content="Francesco Sannicola Portfolio: software engineering and artificial intelligence."
    />
    <meta
      property="og:url"
      content="https://www.francescosannicola.it/articles"
    />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="./assets/francescosannicola.jpg" />
    <title>
      Francesco Sannicola - Gen AI and LLM Interview Questions for Top Companies
    </title>
    <link rel="icon" type="image/x-icon" href="../favicon.ico" />
    <script
      type="text/javascript"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="stylesheet" type="text/css" href="../style.css" />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="main-container">
      <div class="left-column">
        <header>
          <img
            src="../assets/francescosannicola-black.jpg"
            class="profile"
            alt="Profile Picture"
          />
          <div>
            <h1>Francesco Sannicola</h1>
            <h4 class="role">
              Machine Learning | Software Engineering | Cloud Computing
            </h4>
            <div class="contact-info">
              <a
                href="https://d1jdvilibzz056.cloudfront.net/curriculum.pdf"
                target="_blank"
              >
                <svg
                  fill="#000000"
                  version="1.1"
                  id="Capa_1"
                  xmlns="http://www.w3.org/2000/svg"
                  xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="18"
                  height="18"
                  viewBox="0 0 45.057 45.057"
                  xml:space="preserve"
                >
                  <title>Curriculum</title>
                  <g>
                    <g id="_x35_8_24_">
                      <g>
                        <path
                          d="M19.558,25.389c-0.067,0.176-0.155,0.328-0.264,0.455c-0.108,0.129-0.24,0.229-0.396,0.301
                                        c-0.156,0.072-0.347,0.107-0.57,0.107c-0.313,0-0.572-0.068-0.78-0.203c-0.208-0.137-0.374-0.316-0.498-0.541
                                        c-0.124-0.223-0.214-0.477-0.27-0.756c-0.057-0.279-0.084-0.564-0.084-0.852c0-0.289,0.027-0.572,0.084-0.853
                                        c0.056-0.281,0.146-0.533,0.27-0.756c0.124-0.225,0.29-0.404,0.498-0.541c0.208-0.137,0.468-0.203,0.78-0.203
                                        c0.271,0,0.494,0.051,0.666,0.154c0.172,0.105,0.31,0.225,0.414,0.361c0.104,0.137,0.176,0.273,0.216,0.414
                                        c0.04,0.139,0.068,0.25,0.084,0.33h2.568c-0.112-1.08-0.49-1.914-1.135-2.502c-0.644-0.588-1.558-0.887-2.741-0.895
                                        c-0.664,0-1.263,0.107-1.794,0.324c-0.532,0.215-0.988,0.52-1.368,0.912c-0.38,0.392-0.672,0.863-0.876,1.416
                                        c-0.204,0.551-0.307,1.165-0.307,1.836c0,0.631,0.097,1.223,0.288,1.77c0.192,0.549,0.475,1.021,0.847,1.422
                                        s0.825,0.717,1.361,0.949c0.536,0.23,1.152,0.348,1.849,0.348c0.624,0,1.18-0.105,1.668-0.312
                                        c0.487-0.209,0.897-0.482,1.229-0.822s0.584-0.723,0.756-1.146c0.172-0.422,0.259-0.852,0.259-1.283h-2.593
                                        C19.68,25.023,19.627,25.214,19.558,25.389z"
                        />
                        <polygon
                          points="26.62,24.812 26.596,24.812 25.192,19.616 22.528,19.616 25.084,28.184 28.036,28.184 30.713,19.616 28,19.616 
                                        "
                        />
                        <path
                          d="M33.431,0H5.179v45.057h34.699V6.251L33.431,0z M36.878,42.056H8.179V3h23.706v4.76h4.992L36.878,42.056L36.878,42.056z"
                        />
                      </g>
                    </g>
                  </g>
                </svg>
              </a>
              <a href="mailto:francescosannicola1997@gmail.com">
                <svg
                  width="18"
                  height="21"
                  fill="#000000"
                  viewBox="0 0 1920 1920"
                  xmlns="http://www.w3.org/2000/svg"
                >
                  <title>Email</title>
                  <path
                    d="M0 1694.235h1920V226H0v1468.235ZM112.941 376.664V338.94H1807.06v37.723L960 1111.233l-847.059-734.57ZM1807.06 526.198v950.513l-351.134-438.89-88.32 70.475 378.353 472.998H174.042l378.353-472.998-88.32-70.475-351.134 438.89V526.198L960 1260.768l847.059-734.57Z"
                    fill-rule="evenodd"
                  />
                </svg>
              </a>
              <a
                href="https://www.linkedin.com/in/francesco-sannicola"
                target="_blank"
              >
                <svg
                  fill="#000000"
                  width="18"
                  height="18"
                  viewBox="0 0 1920 1920"
                  xmlns="http://www.w3.org/2000/svg"
                >
                  <title>LinkedIn</title>
                  <path
                    d="M1168 601.321v74.955c72.312-44.925 155.796-71.11 282.643-71.11 412.852 0 465.705 308.588 465.705 577.417v733.213L1438.991 1920v-701.261c0-117.718-42.162-140.06-120.12-140.06-74.114 0-120.12 23.423-120.12 140.06V1920l-483.604-4.204V601.32H1168Zm-687.52-.792v1318.918H0V600.53h480.48Zm-120.12 120.12H120.12v1078.678h240.24V720.65Zm687.52.792H835.267v1075.316l243.364 2.162v-580.18c0-226.427 150.51-260.18 240.24-260.18 109.55 0 240.24 45.165 240.24 260.18v580.18l237.117-2.162v-614.174c0-333.334-93.573-457.298-345.585-457.298-151.472 0-217.057 44.925-281.322 98.98l-16.696 14.173H1047.88V721.441ZM240.24 0c132.493 0 240.24 107.748 240.24 240.24 0 132.493-107.747 240.24-240.24 240.24C107.748 480.48 0 372.733 0 240.24 0 107.748 107.748 0 240.24 0Zm0 120.12c-66.186 0-120.12 53.934-120.12 120.12s53.934 120.12 120.12 120.12 120.12-53.934 120.12-120.12-53.934-120.12-120.12-120.12Z"
                    fill-rule="evenodd"
                  />
                </svg>
              </a>
              <a href="https://github.com/francesco-s" target="_blank">
                <svg
                  fill="#000000"
                  width="20"
                  height="20"
                  viewBox="0 0 32 32"
                  version="1.1"
                  xmlns="http://www.w3.org/2000/svg"
                >
                  <title>Github</title>
                  <path
                    d="M18.36 9.28c0.48-1.72-0.24-3.6-0.28-3.72-0.12-0.28-0.4-0.52-0.72-0.52-0.080 0-1.92-0.16-3.76 1.24-1.44-0.28-3.080-0.36-3.16-0.36-0.040 0-0.040 0-0.080 0-0.080 0-1.72 0.080-3.16 0.36-1.84-1.4-3.68-1.24-3.76-1.24-0.32 0.040-0.6 0.24-0.72 0.52-0.040 0.080-0.8 2-0.28 3.72-0.92 1.28-1.64 2.96-1 5.96 0.6 2.72 2.84 4.24 5.16 4.76-0.2 0.56-0.28 1.24-0.36 1.96-0.96 0.040-1.56-0.52-2.4-1.4-0.72-0.76-1.52-1.64-2.84-1.92-0.44-0.12-0.88 0.16-1 0.64-0.080 0.48 0.2 0.92 0.68 1 0.76 0.16 1.28 0.72 1.92 1.4 0.84 0.88 1.8 1.96 3.52 1.96 0 0 0.040 0 0.040 0 0 0.92 0.080 1.8 0.12 2.52 0.040 0.48 0.44 0.8 0.92 0.76s0.8-0.44 0.76-0.92c-0.24-2.72-0.040-5.6 0.4-6 0.32-0.2 0.52-0.56 0.4-0.96-0.080-0.36-0.4-0.64-0.8-0.64-0.36 0-4.12-0.2-4.84-3.52-0.6-2.72 0.16-3.92 0.96-4.88 0.2-0.24 0.24-0.6 0.12-0.92-0.32-0.68-0.2-1.64-0.040-2.28 0.56 0.080 1.4 0.32 2.28 1.080 0.2 0.2 0.48 0.24 0.76 0.2 1.24-0.32 2.92-0.4 3.2-0.4 0.24 0 1.96 0.080 3.2 0.4 0.28 0.080 0.56 0 0.76-0.2 0.88-0.76 1.76-1 2.28-1.080 0.16 0.6 0.28 1.56-0.040 2.28-0.12 0.28-0.080 0.64 0.12 0.92 0.8 0.96 1.52 2.16 0.96 4.88-0.72 3.32-4.48 3.52-4.92 3.56-0.4 0-0.72 0.28-0.8 0.64s0.080 0.76 0.4 0.96c0.48 0.4 0.68 3.24 0.44 6-0.040 0.48 0.32 0.88 0.76 0.92 0.040 0 0.040 0 0.080 0 0.44 0 0.8-0.32 0.84-0.76 0.16-1.76 0.28-4.48-0.28-6.2 2.32-0.48 4.56-2.040 5.16-4.76 0.64-3-0.040-4.68-1-5.96z"
                  ></path>
                </svg>
              </a>
            </div>
          </div>
          <div class="switch">
            <input type="checkbox" id="darkModeToggle" />
            <label for="darkModeToggle" class="switch-label">
              <span class="sun-icon">&#9728;</span>
              <!-- Sun icon (Light Mode) -->
              <span class="moon-icon">&#9790;</span>
              <!-- Moon icon (Dark Mode) -->
            </label>
          </div>
        </header>
      </div>
    </div>
    <div class="link-container">
      <div class="link-column">
        <a href="../index.html"> <i class="fas fa-home"></i>Home </a>
      </div>
      <div class="link-column">
        <a href="../projects/overview.html">
          <i class="fas fa-project-diagram"></i>Projects
        </a>
      </div>
      <div class="link-column">
        <!--<a href="https://d1jdvilibzz056.cloudfront.net/articles/">-->
        <a href="overview.html"> <i class="fas fa-newspaper"></i>Articles </a>
      </div>
      <div class="link-column">
        <a href="../about.html"> <i class="fas fa-user"></i>About </a>
      </div>
    </div>
    <div class="article-container">
      <h1>
        <span style="font-size: 16px">./articles/</span> GenAI and LLM Interview
        Questions with In-Depth Answers
      </h1>

      <p>
        The rapid advancements in Generative AI and Large Language Models (LLMs)
        have brought transformative changes to industries ranging from customer
        support to creative content generation. As organizations and
        professionals dive into this exciting domain, understanding the key
        concepts and gaining practical knowledge is crucial.
      </p>
      <p>
        This guide serves as a comprehensive resource for anyone preparing for
        interviews or aiming to deepen their expertise in LLMs. Each topic is
        composed of a wide range of questions, carefully curated to cover
        foundational concepts, practical applications, and advanced topics. The
        questions in this guide are derived from the repository
        <a
          href="https://github.com/llmgenai/LLMInterviewQuestions/tree/main?tab=readme-ov-file#deployment-of-llm"
        >
          LLM Interview Questions</a
        >, a community-driven resource dedicated to fostering knowledge and
        preparation in the field of LLMs.
      </p>

      <p>
        The questions tackle diverse areas such as Prompt Engineering and Basics
        of LLM, exploring foundational principles like understanding predictive
        vs. generative AI, key concepts in language model training, and decoding
        strategies, as well as advanced aspects like in-context learning and
        strategies to optimize prompt writing for enhanced model performance.
        These topics highlight not only theoretical knowledge but also practical
        approaches to improving the reasoning and utility of LLMs across varied
        use cases.
      </p>

      <div class="summary">
        <h4>Quick Navigation</h4>
        <ol>
          <li><a href="#llm-basics">Basics of LLM</a></li>
          <li><a href="#prompt-engineering">Prompt Engineering</a></li>
          <li><a href="#rag">Retrieval Augmented Generation (RAG)</a></li>
          <li><a href="#fine-tuning">Fine-tuning of LLMs</a></li>
          <li><a href="#agent-based">Agent-Based System</a></li>
          <li><a href="#miscellaneous">Miscellaneous</a></li>
        </ol>
      </div>

      <h2 id="llm-basics">1. Basics of LLM</h2>
      <h4>
        1.1 What is the difference between Predictive/Discriminative AI and
        Generative AI?
      </h4>

      <p>
        Predictive AI, also known as discriminative AI, focuses on predicting
        the output based on input data. It aims to learn the relationship
        between input features and output labels, making predictions based on
        patterns in the training data. Common examples of predictive AI include
        classification and regression tasks, where the model predicts discrete
        classes or continuous values, respectively.
      </p>

      <p>Core Characteristics:</p>
      <ul>
        <li>
          Focuses on learning \(P(y|x)\) - the probability of output y given
          input x
        </li>
        <li>Optimized for accuracy in classification and prediction tasks</li>
        <li>Efficient in resource usage compared to generative models</li>
        <li>Typically requires less training data for good performance</li>
        <li>Better suited for tasks with clear decision boundaries</li>
      </ul>
      <p>Key Technical Aspects of Predictive AI you must known:</p>
      <ul>
        <li>
          <strong>Feature Engineering:</strong> The process of manually or
          automatically deriving meaningful features from raw data to improve
          model performance.
        </li>
        <li>
          <strong>Loss Functions:</strong> Metrics used to quantify prediction
          errors, including cross-entropy, hinge loss, and mean squared error
          (MSE).
        </li>
        <li>
          <strong>Optimization:</strong> Techniques such as gradient descent and
          stochastic optimization to minimize the loss function during training.
        </li>
        <li>
          <strong>Regularization:</strong> Methods like L1/L2 regularization,
          dropout, and batch normalization to prevent overfitting and improve
          generalization.
        </li>
      </ul>

      <p>
        Generative AI, on the other hand, is designed to generate new data that
        resembles the training data distribution. Instead of predicting a
        specific output, generative models learn the underlying structure of the
        data and generate new samples that are similar to the training examples.
        These models can create new images, text, audio, or other types of data
        based on the patterns they have learned during training.
      </p>
      <p>Core Characteristics:</p>
      <ul>
        <li>Models the complete joint probability distribution \(P(x,y)\)</li>
        <li>Creates novel, coherent outputs across various domains</li>
        <li>
          Can handle multiple tasks including content generation and
          understanding
        </li>
        <li>Requires larger datasets and computational resources</li>
        <li>Capable of unsupervised and self-supervised learning</li>
      </ul>

      <p>Key Examples of GenAI Architectures to know:</p>
      <ul>
        <li>
          <strong>Transformer Models:</strong> Use self-attention mechanisms,
          which allow the model to weigh the importance of different words in a
          sentence, and multi-head attention, which enables the model to focus
          on different parts of the sentence simultaneously for better context
          understanding.
        </li>
        <li>
          <strong>Generative Adversarial Networks (GANs):</strong> Utilize a
          generator, which creates data samples, and a discriminator, which
          evaluates them, in a competitive framework to create realistic data
          samples.
        </li>
        <li>
          <strong>Diffusion Models:</strong> Use denoising processes, which
          iteratively refine noisy data to generate high-quality outputs, and
          U-Net architectures, which are convolutional neural networks designed
          for image segmentation and generation.
        </li>
        <li>
          <strong>Variational Autoencoders (VAEs):</strong> Focus on latent
          space modeling using the reparameterization trick, which allows for
          backpropagation through stochastic variables by introducing a
          differentiable approximation, and minimize reconstruction loss for
          generative tasks.
        </li>
      </ul>

      <svg
        viewBox="0 0 400 200"
        width="720"
        height="480"
        xmlns="http://www.w3.org/2000/svg"
      >
        <!-- Left side - Discriminative -->
        <g transform="translate(50, 0)">
          <!-- Dots group 1 -->
          <g>
            <circle cx="25" cy="25" r="4" fill="#4299E1" />
            <circle cx="15" cy="40" r="4" fill="#4299E1" />
            <circle cx="35" cy="45" r="4" fill="#4299E1" />
            <circle cx="10" cy="60" r="4" fill="#4299E1" />
            <circle cx="30" cy="70" r="4" fill="#4299E1" />
            <circle cx="20" cy="80" r="4" fill="#4299E1" />
            <circle cx="40" cy="85" r="4" fill="#4299E1" />
          </g>
          <!-- Dots group 2 -->
          <g>
            <circle cx="90" cy="25" r="4" fill="#2B6CB0" />
            <circle cx="75" cy="35" r="4" fill="#2B6CB0" />
            <circle cx="95" cy="50" r="4" fill="#2B6CB0" />
            <circle cx="80" cy="65" r="4" fill="#2B6CB0" />
            <circle cx="100" cy="75" r="4" fill="#2B6CB0" />
            <circle cx="85" cy="85" r="4" fill="#2B6CB0" />
            <circle cx="70" cy="95" r="4" fill="#2B6CB0" />
          </g>
          <!-- Decision boundary -->
          <line
            x1="50"
            y1="10"
            x2="55"
            y2="100"
            stroke="black"
            stroke-width="1"
            stroke-dasharray="4,3"
          />
          <!-- Label -->
          <text
            x="50"
            y="125"
            text-anchor="middle"
            font-size="10"
            font-family="Arial"
          >
            Discriminative
          </text>
        </g>

        <!-- Right side - Generative -->
        <g transform="translate(225,0)">
          <!-- Container 1 -->
          <rect
            x="0"
            y="20"
            width="60"
            height="90"
            rx="30"
            fill="#4299E1"
            opacity="0.3"
          />
          <g>
            <circle cx="15" cy="35" r="4" fill="#4299E1" />
            <circle cx="30" cy="45" r="4" fill="#4299E1" />
            <circle cx="20" cy="60" r="4" fill="#4299E1" />
            <circle cx="40" cy="70" r="4" fill="#4299E1" />
            <circle cx="25" cy="85" r="4" fill="#4299E1" />
            <circle cx="35" cy="95" r="4" fill="#4299E1" />
          </g>
          <!-- Container 2 -->
          <rect
            x="70"
            y="20"
            width="60"
            height="90"
            rx="30"
            fill="#2B6CB0"
            opacity="0.3"
          />
          <g>
            <circle cx="85" cy="35" r="4" fill="#2B6CB0" />
            <circle cx="100" cy="45" r="4" fill="#2B6CB0" />
            <circle cx="90" cy="60" r="4" fill="#2B6CB0" />
            <circle cx="110" cy="70" r="4" fill="#2B6CB0" />
            <circle cx="95" cy="85" r="4" fill="#2B6CB0" />
            <circle cx="105" cy="95" r="4" fill="#2B6CB0" />
          </g>
          <!-- Label -->
          <text
            x="65"
            y="125"
            text-anchor="middle"
            font-size="10"
            font-family="Arial"
          >
            Generative
          </text>
        </g>
      </svg>

      <h4>1.2 What is LLM, and how are LLMs trained?</h4>
      <p>
        LLM (Large Language Model) is an AI system trained to understand and
        generate human-like text by processing vast amounts of textual data.
        These models represent the cutting edge of natural language processing
        technology.
      </p>
      <p>
        Training large language models is a multi-layered stack of processes,
        each playing a unique role in shaping the model's performance. The three
        main phases are:
      </p>
      <ul>
        <li>Self-supervised learning</li>
        <li>Supervised learning</li>
        <li>Reinforcement learning</li>
      </ul>
      <p>
        <strong
          >Phase 1: Self-Supervised Learning for Language Understanding</strong
        >
      </p>
      <p>
        Self-supervised learning, the first phase of training, is typically what
        comes to mind when discussing language modeling. This process involves
        exposing the model to vast amounts of unannotated or raw data and
        instructing it to predict the ‘missing’ elements within that data.
        Through this, the model learns about both language and the underlying
        domain to generate plausible responses.
      </p>

      <p>
        For instance, if we provide the model with text from a weather website
        and ask it to predict the next word, the model must comprehend the
        language and the context of the weather domain. In my presentation, I
        used the example: “A flash flood watch will be in effect all _____.” At
        an intermediate stage, the model ranks possible predictions, from the
        most likely answers (“day,” “night,” “hour”) to those that are less
        probable (“month”), and even to nonsensical ones (“giraffe”) that
        receive low probability scores.
      </p>
      <p>
        This process is called self-supervision (distinct from unsupervised
        learning) because there is a specific, correct answer—the word that
        appeared in the collected text, which in this case was “night.” While
        self-supervision shares similarities with unsupervised learning, it is
        distinct in that it focuses on predicting specific correct outcomes,
        even within the context of abundant, unannotated data.
      </p>

      <img
        src="../assets/self-supervised-learning.png"
        alt="Visualization of LLM training process"
        class="responsive-img"
        width="720"
        height="480"
      />

      <p>
        <strong
          >Phase 2: Supervised Learning for Instruction Understanding</strong
        >
      </p>

      <p>
        Supervised learning, or instruction tuning, marks the second stage in
        the training process of large language models (LLMs). This phase is
        vital, building upon the foundational knowledge established during the
        self-supervised learning phase. In this phase, the model is trained
        specifically to follow instructions. Unlike self-supervised learning,
        which focuses on predicting words and completing sentences, instruction
        tuning teaches the model to understand and respond to explicit user
        requests. This shift makes the model significantly more interactive and
        useful in real-world applications. The impact of instruction tuning on
        enhancing LLM capabilities has been demonstrated through numerous
        studies, including those led by Snorkel researchers. One key outcome was
        that models trained with instruction tuning performed better at
        generalizing to new, unseen tasks. This is a major achievement, as the
        ability to effectively handle unfamiliar tasks is a central goal of
        machine learning models. Given its proven success, instruction tuning
        has become a standard practice in LLM training. Once this phase is
        completed, the model is no longer just predicting the next word—it’s
        trained to engage with users, comprehend their instructions, and provide
        meaningful, context-aware responses.
      </p>

      <p>
        <strong
          >Phase 3: Reinforcement Learning to Promote Desired Behavior</strong
        >
      </p>
      <p>
        The final stage in training large language models (LLMs) is
        reinforcement learning, a critical process that fine-tunes the model by
        encouraging desired behaviors and discouraging undesirable outputs.
        Unlike earlier stages, this phase doesn’t provide the model with
        specific target outputs but instead evaluates and “grades” the responses
        the model generates.
      </p>
      <p>
        Although reinforcement learning has a long history in machine learning,
        its application to LLM training was first proposed by OpenAI after the
        advent of instruction tuning. The process begins with a model already
        capable of understanding instructions and generating coherent language.
        Human annotations are then used to compare model outputs, identifying
        which responses are better or worse. These comparisons help guide the
        creation of a reward model, which assigns quantitative scores to the
        quality of the outputs. The reward model plays a pivotal role by scaling
        feedback to the model, steering it toward generating preferred responses
        while penalizing undesired ones. This method is particularly effective
        for fostering nuanced behaviors, such as encouraging brevity or
        discouraging harmful or inappropriate language. As a result, the model
        learns to produce higher-quality, context-sensitive responses.
      </p>
      <p>
        This process, known as reinforcement learning with human feedback
        (RLHF), underscores the value of human involvement in shaping the
        model's behavior. By incorporating human preferences into the training
        loop, RLHF ensures the model aligns more closely with user expectations
        and ethical standards, delivering a safer and more user-centric
        experience.
      </p>

      <img
        src="../assets/llm-training.png"
        alt="Visualization of training process"
        class="responsive-img"
        width="720"
        height="480"
      />

      <h4>1.3 What is a token in the language model?</h4>
      <p>
        In the context of language models, a token refers to the smallest unit
        of text that the model processes. Tokens can represent individual words,
        subwords, or characters, depending on the tokenization strategy used.
        The choice of tokenization impacts the model's ability to understand and
        generate text.
      </p>
      <p>
        Each language model has a context window—a limit on the number of tokens
        it can handle at a time. This context window defines how much text the
        model can "remember" and process simultaneously. For example, a model
        with a 4,000-token context window can work with approximately
        3,000–4,000 words of text at once, depending on the complexity of the
        language and the average token length.
      </p>
      <p>
        The vector representation of a token is called an embedding. These
        vectors capture the meaning, context, and relationships between tokens
        in a form that the model can compute.
      </p>

      <h4>
        1.4 How to estimate the cost of running SaaS-based and Open Source LLM
        models?
      </h4>

      <p>
        These are subscription-based or pay-per-use services that provide hosted
        LLMs. SaaS-based LLM services charge based on processed tokens (both
        input and output). For example, GPT-4 charges $0.03 for input tokens and
        $0.06 for output tokens per 1,000 tokens. The total cost can be
        calculated by multiplying the sum of input and output tokens by the cost
        per token.
      </p>
      <p class="responsive-math">
        Total Cost = (Tokens Input + Tokens Output) × Cost Per Token
      </p>
      <p>
        Other costs include compute tier options (with different pricing for
        different models), monthly subscription fees for premium access, and
        possible additional charges for reserved capacity to ensure low latency
        or scalability.
      </p>
      <p>
        With SaaS, there’s no need for infrastructure setup or maintenance. The
        service scales automatically, making it ideal for prototypes or
        low-volume usage.
      </p>

      <p>
        On the other hand, Open Source LLM models are free to use but require
        infrastructure setup and maintenance. For self-hosted open-source
        models, compute costs can vary widely depending on the choice of
        infrastructure. Cloud-based GPU instances range from $1 to $6 per hour
        per GPU, with an average cost for an NVIDIA A100 instance being around
        $2.5 per hour. The compute cost is calculated by multiplying the
        instance cost by the number of hours used.
      </p>
      <p class="responsive-math">
        Cost (Compute) = (Instance Cost) × (Hours of Use)
      </p>
      <p>
        Open-source LLMs also incur storage and networking costs (e.g., 100GB+
        for large models), energy costs for continuous operation, personnel
        costs for engineering and maintenance, and possible software licensing
        fees for supporting software.
      </p>
      <p>
        Open source LLMs offer full control over the model and infrastructure.
        They can be customized for specific needs and may incur lower long-term
        costs for high-volume usage.
      </p>

      <p><strong>Comparison table</strong></p>
      <div class="responsive-table-container">
        <table border="1">
          <thead>
            <tr>
              <th>Factor</th>
              <th>SaaS-Based</th>
              <th>Open-Source</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Ease of Setup</td>
              <td>Instant, minimal configuration</td>
              <td>Significant setup and maintenance</td>
            </tr>
            <tr>
              <td>Scalability</td>
              <td>Automatic, scales with need</td>
              <td>Manual provisioning required</td>
            </tr>
            <tr>
              <td>Costs for Low Usage</td>
              <td>Lower for prototypes</td>
              <td>Higher initial setup cost</td>
            </tr>
            <tr>
              <td>Costs for High Usage</td>
              <td>Scales linearly, can be expensive</td>
              <td>More cost-effective long-term</td>
            </tr>
            <tr>
              <td>Customizability</td>
              <td>Limited by vendor</td>
              <td>Full control</td>
            </tr>
            <tr>
              <td>Operational Control</td>
              <td>Relies on vendor</td>
              <td>Complete independence</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h4>1.5 Explain the Temperature parameter and how to set it.</h4>

      <p>
        The <strong>Temperature</strong> parameter is used in language models to
        control the level of randomness in the text generation process. It is
        particularly useful for fine-tuning the creativity and variability of
        the model’s responses.
      </p>

      <p>
        In simple terms, the temperature adjusts the "steepness" of the
        probability distribution over possible next words or tokens. When
        generating text, a model selects tokens based on a distribution of
        probabilities. A lower temperature (e.g., 0.1) makes the model more
        deterministic, meaning it is more likely to select the most probable
        next token, leading to more predictable and coherent text. A higher
        temperature (e.g., 1.0) adds more randomness to the selection, allowing
        for more creative or unexpected outputs.
      </p>

      <p>
        The temperature parameter works by modifying the logits (raw scores) of
        the possible next tokens before converting them to probabilities using
        the softmax function. The formula for calculating the probability
        distribution for the next token given the logits is:
      </p>

      <p class="responsive-math">
        \[P(w_i) = exp(logit(w_i) / T) / Σ exp(logit(w_j) / T) \]
      </p>

      <p>
        Here, <span>\(w_i\)</span> refers to the ith token in the vocabulary,
        and <span>\(logit(w_i)\)</span> is the unscaled score for that token.
        The parameter <em>T</em> is the temperature:
      </p>
      <ul>
        <li>
          T = 0: This means maximum determinism. The model always selects the
          most probable token (argmax sampling). However, most implementations
          prevent using an exact 0 since it removes any stochasticity
          altogether.
        </li>
        <li>
          T between 0 and 1: Sharper probability distributions, making
          high-probability tokens even more dominant. Lower values reduce
          randomness and make the output more focused and predictable.
        </li>
        <li>
          T = 1: No adjustment to the logits. The original probability
          distribution is used.
        </li>
        <li>
          T > 1: Flattens the probability distribution, increasing randomness.
          Less probable tokens become more likely to be chosen, producing
          creative or exploratory outputs.
        </li>
      </ul>
      <img
        src="../assets/temperature.png"
        alt="Temperature parameter visualization"
        class="responsive-img"
        width="720"
        height="480"
      />

      <p>
        The temperature parameter is often adjusted through an API or in the
        model configuration. If you are interacting with an API, you will
        usually specify the temperature in the request. Here's a typical API
        call setting:
      </p>

      <pre>
      {
        "model": "gpt-4",
        "temperature": 0.7,
        "prompt": "Describe a sunset over a mountain range."
      }
      </pre>

      <p>
        Experimenting with different temperature settings can help achieve the
        desired tone and level of creativity in the responses. Lower
        temperatures are appropriate for factual, straightforward tasks, while
        higher temperatures are useful for generating new ideas, brainstorming,
        or creative writing.
      </p>

      <h4>
        1.6 What are different decoding strategies for picking output tokens?
      </h4>
      <p>
        Language models are pre-trained to predict the next token in a text
        corpus. Decoding strategies determine how to select the next token based
        on the probability distribution over a fixed vocabulary. The process of
        selecting these tokens, known as decoding, plays a crucial role in
        shaping the output text. By tailoring the decoding approach, you can
        customize text generation to suit specific needs. Depending on the
        decoding method, the model may choose the most probable token, consider
        multiple top candidates, or introduce randomness for variety.
      </p>
      <p>
        An effective decoding strategy transforms a language model from a simple
        next-token predictor into a powerful text generator capable of handling
        diverse tasks. This raises two key questions: "What are the different
        decoding strategies?" and "How do they influence the output generated by
        language models?".
      </p>
      <img
        src="../assets/autoregressive_gen.webp"
        alt="Autoregressive generation"
        class="responsive-img"
        width="720"
        height="480"
      />

      <p>
        The simplest way to implement a sampling function is to select the next
        token with the highest probability at each step, an approach known as
        <strong>Greedy Search</strong>. This straightforward method is fast and
        efficient because it prioritizes the most probable token every time.
        However, this predictability often results in repetitive or unoriginal
        text, making it unsuitable for generating creative content. Can this be
        improved? Absolutely.
      </p>
      <p>
        <strong>Beam search</strong> offers an enhancement by maintaining a beam
        of the K most probable sequences at each time step, where K is the beam
        width. The process continues until a maximum sequence length is reached
        or an end-of-sequence token is generated. Beam search typically results
        in higher-quality text than greedy search, but it requires more
        computational effort.
      </p>
      <p>
        Suppose the initial sequence is "Once upon a time" and
        <span class="highlight">K=2</span>.
      </p>
      <ul>
        <li>
          The two most likely tokens are <strong>"a"</strong> and
          <strong>"the"</strong>.
        </li>
        <li>
          In the next step:
          <ul>
            <li>
              The sequence <em>("a", "cat")</em> has a probability of
              <strong>0.20</strong> (<code>0.5 × 0.4</code>).
            </li>
            <li>
              The sequence <em>("the", "people")</em> has a probability of
              <strong>0.21</strong> (<code>0.3 × 0.7</code>).
            </li>
          </ul>
        </li>
        <li>
          Beam search selects the sequence <em>"the people"</em> as it has the
          higher cumulative probability.
        </li>
      </ul>
      <img
        src="../assets/beam_search.webp"
        alt="Beam search visualization"
        class="responsive-img"
        width="720"
        height="480"
      />

      <p>
        The above methods that choose the most probable next token at each step
        are called Deterministic methods. These methods produce output text
        ensuring predictability but often at the expense of diversity. So if you
        are looking for creative writing, we need some better mechanisms.
      </p>
      <p>
        To overcome the limitations of deterministic methods in generating
        varied and creative text, stochastic methods introduce randomness into
        the selection process. These methods reduce predictability, creating
        outputs that are less repetitive and more diverse.
      </p>
      <p>
        When generating text, stochastic methods often rely on strategies that
        prioritize tokens with higher probabilities while maintaining a degree
        of randomness. Popular approaches for achieving this balance are ramdom
        sampling, temperature sampling, Top-k sampling and Top-p sampling, each
        offering unique mechanisms for selecting the next token.
      </p>
      <p>
        The simplest stochastic approach is <strong>random sampling</strong>,
        where the next token is sampled from the probability distribution:
      </p>
      <pre>
        <code class="language-python">
        def sample(p):
            return np.random.choice(np.arange(p.shape[-1]), p=p)</code>
            </pre>
      <p>
        With this method, each execution produces different outputs. However,
        while less predictable, this approach may result in incoherent text. To
        achieve varied yet coherent results, we need more refined strategies.
      </p>
      <p>
        <strong>Temperature sampling</strong>, as discuss earlier, adjusts the
        likelihood of selecting tokens by altering the temperature of the
        softmax function, which transforms the model’s logits into
        probabilities.
      </p>
      <p>
        Consider tokens with logits [1, 2, 3, 4, 5]. The following code plots
        the temperature-controlled softmax probability distribution:
      </p>
      <pre>
        <code class="language-python">
        from matplotlib import pyplot
        import torch
        
        depth = range(6)
        logits = torch.tensor([1, 2, 3, 4, 5])
        prob_list = []
        
        for temperature in depth:
            prob = torch.nn.functional.softmax(logits / (temperature + 0.1), dim=-1)
            prob_list.append(prob.numpy())
        
        pyplot.plot(depth, prob_list)
        pyplot.xlabel("Temperature")
        pyplot.ylabel("Probability")
        pyplot.title("Temperature-Controlled Softmax Distribution")
        pyplot.show()</code>
            </pre>

      <img
        src="../assets/visualization-temperature.webp"
        alt="Temperature sampling visualization"
        class="responsive-img"
        width="620"
        height="360"
      />

      <p>
        <strong>Top-k sampling</strong> limits selection to the
        <strong>k</strong> most probable tokens. For instance, if
        <span class="math">k=2</span> and token probabilities are:
      </p>
      <p class="responsive-math">
        \( P(T_0) = 0.6, P(T_1) = 0.4, P(T_2) = 0.3, P(T_3) = 0.1, P(T_4) =
        0.05, P(T_5) = 0.04 \)
      </p>

      <p>
        The tokens <span class="math">\( T_3, T_4, T_5 \)</span> are excluded,
        and the remaining probabilities are redistributed.
      </p>
      <img
        src="../assets/topk.png"
        alt="Top-k sampling visualization"
        class="responsive-img"
        width="320"
        height="160"
      />

      <p>
        <strong>Top-p Sampling</strong> — chooses from the possible set of words
        whose cumulative probability exceeds the probability p. The probability
        mass is then redistributed among this set of words. For p = 0.95, Top-p
        sampling picks the minimum number of tokens to exceed together p = 95%
        of the probability mass. Unlike Top-k sampling which has a fixed number
        of tokens k, this mechanism allows the number of tokens in a set to
        dynamically increase or decrease according to the next word’s
        probability distribution. Choosing a set of high-probability tokens
        removes the very unlikely low-probability values, thus helping to
        generate diverse and coherent text, making it very popular for text
        generation. This Top-p sampling is also known as Nucleus sampling.
      </p>
      <img
        src="../assets/topp.png"
        alt="Top-p sampling visualization"
        class="responsive-img"
        width="320"
        height="160"
      />
      <p>
        Combining top-p and top-k sampling can further refine the balance
        between randomness and coherence.
      </p>

      <h4>
        1.7 What are different ways you can define stopping criteria in large
        language model?
      </h4>

      <p>
        One common approach is to set a <strong>predefined token limit</strong>.
        By specifying the maximum length of the output, we can ensure that the
        generated text doesn’t exceed a certain number of tokens. This is
        particularly useful in cases like generating headlines or summaries
        where brevity is essential. For more structured tasks,
        <strong>minimum and maximum length thresholds</strong> can be combined.
        This ensures the output is neither too short to be meaningful nor too
        long to remain concise. For example, when generating summaries, you
        might require the output to be at least 30 tokens but no more than 100
        tokens long.
      </p>

      <pre>
        <code class="language-python">
          response = model.generate(input, min_length=30, max_length=100)
        </code></pre>

      <p>
        Another widely used technique involves relying on the
        <strong>end-of-sequence (EOS) token</strong>. The model generates tokens
        until it predicts this special token, which signals that the sequence
        has concluded naturally. This method is ideal for scenarios like
        document generation or conversational models, where the model's training
        data inherently includes a concept of when sequences should terminate.
      </p>
      <pre>
        <code class="language-python">
          if token == eos_token:
            break
      </code></pre>

      <p>
        To prevent outputs from becoming repetitive or looping endlessly, a
        <strong>repetition-based stopping criterion</strong> or
        <strong>frequency penalty</strong> is often applied. This involves
        checking the sequence of tokens generated so far and ending the process
        if significant repetition is detected.
      </p>
      <pre>
        <code class="language-python">
          if sequence in history:
            break
          </code></pre>

      <p>
        In dynamic and real-time applications like chatbots, a
        <strong>time-based stopping criterion</strong> might be more
        appropriate. Here, the model ceases generation once a specified duration
        has passed. This ensures timely responses and enhances the interactive
        experience for users.
      </p>

      <p>
        Another advanced approach involves <strong>semantic analysis</strong>.
        Instead of setting hard rules based on token counts or repetition, the
        model evaluates the coherence and relevance of its generated tokens. If
        the semantic quality drops below a specific threshold, the generation is
        stopped. <strong>Probabilistic methods</strong> can also guide stopping
        criteria. For instance, by monitoring the probabilities associated with
        predicted tokens, you could stop when the highest-probability token
        falls below a certain threshold, indicating that the model is uncertain.
        This method is useful in precision-critical tasks.
      </p>

      <p>
        In many cases, a <strong>combination of criteria</strong> yields the
        best results. For example, combining a token limit, EOS token detection,
        and repetition-based stopping can create robust rules for most tasks.
      </p>

      <h4>1.8 How to use stop sequences in LLMs?</h4>
      <p>
        Once you've defined the stop sequence, you'll typically pass it as a
        parameter when making the request to generate text from an LLM API.
      </p>
      <p>
        If you’re using OpenAI's API to generate text, you can specify the stop
        parameter in the API request.
      </p>
      <pre>
        <code class="language-python">
          import openai

          response = openai.Completion.create(
              engine="text-davinci-003",  # The model to use
              prompt="Please write a creative ending for Dragon Ball.",  # The text prompt
              stop=["END", "===END==="],  # List of stop sequences to terminate the generation
              max_tokens=100      # Limit the maximum number of tokens (words/pieces of text)
          )

          print(response.choices[0].text.strip())
        </code></pre>

      <h2 id="prompt-engineering">2. Prompt Engineering</h2>
      --------------------------------------------------

      <h4>2.1 Explain the basic structure of prompt engineering.</h4>
      <p>
        Effective prompting can unlock the full potential of large language
        models (LLMs). While simple prompts can yield results, the quality of
        the output improves with well-crafted instructions and sufficient
        context.
      </p>
      <p>A prompt contains any of the following elements:</p>
      <ul>
        <li>
          Instruction - a specific task or instruction you want the model to
          perform
        </li>
        <li>
          Context - external information or additional context that can steer
          the model to better responses
        </li>
        <li>
          Input Data - the input or question that we are interested to find a
          response for
        </li>
        <li>Output Indicator - the type or format of the output.</li>
      </ul>
      <p>
        To demonstrate the prompt elements better, here is a simple prompt that
        aims to perform a text classification task:
      </p>
      <div class="responsive-code-container">
        <pre><code>
          Prompt: Classify the text into neutral, negative, or positive Text: I think the food was okay. Sentiment:
        </code></pre>
      </div>
      <p>
        In the prompt example above, the instruction corresponds to the
        classification task, "Classify the text into neutral, negative, or
        positive". The input data corresponds to the "I think the food was
        okay." part, and the output indicator used is "Sentiment:". Note that
        this basic example doesn't use context but this can also be provided as
        part of the prompt. For instance, the context for this text
        classification prompt can be additional examples provided as part of the
        prompt to help the model better understand the task and steer the type
        of outputs that you expect. You do not need all four elements for a
        prompt and the format depends on the task at hand. We will touch on more
        concrete examples in upcoming guides.
      </p>
      <p>
        As you get started with designing prompts, you should keep in mind that
        it is really an iterative process that requires a lot of experimentation
        to get optimal results. Using a simple playground from OpenAI or Cohere
        is a good starting point. You can start with simple prompts and keep
        adding more elements and context as you aim for better results.
        Iterating your prompt along the way is vital for this reason. As you
        read the guide, you will see many examples where specificity,
        simplicity, and conciseness will often give you better results. When you
        have a big task that involves many different subtasks, you can try to
        break down the task into simpler subtasks and keep building up as you
        get better results. This avoids adding too much complexity to the prompt
        design process at the beginning.
      </p>

      <p>
        You can design effective prompts for various simple tasks by using
        commands to instruct the model what you want to achieve, such as
        "Write", "Classify", "Summarize", "Translate", "Order", etc. Keep in
        mind that you also need to experiment a lot to see what works best. Try
        different instructions with different keywords, contexts, and data and
        see what works best for your particular use case and task. Usually, the
        more specific and relevant the context is to the task you are trying to
        perform, the better. We will touch on the importance of sampling and
        adding more context in the upcoming guides. Others recommend that you
        place instructions at the beginning of the prompt. Another
        recommendation is to use some clear separator like "###" to separate the
        instruction and context.
      </p>
      <p>Prompt:</p>
      <div class="responsive-code-container">
        <pre><code>
          ### Instruction ###
          Translate the text below to Spanish:
          Text: "hello!"
        </code></pre>
      </div>
      <p>Output: <code>¡Hola!</code></p>

      <p>
        Be very specific about the instruction and task you want the model to
        perform. The more descriptive and detailed the prompt is, the better the
        results. This is particularly important when you have a desired outcome
        or style of generation you are seeking. There aren't specific tokens or
        keywords that lead to better results. It's more important to have a good
        format and descriptive prompt. In fact, providing examples in the prompt
        is very effective to get desired output in specific formats. When
        designing prompts, you should also keep in mind the length of the prompt
        as there are limitations regarding how long the prompt can be. Thinking
        about how specific and detailed you should be. Including too many
        unnecessary details is not necessarily a good approach. The details
        should be relevant and contribute to the task at hand. This is something
        you will need to experiment with a lot. We encourage a lot of
        experimentation and iteration to optimize prompts for your applications.
      </p>
      <p>Prompt:</p>
      <div class="responsive-code-container">
        <pre><code>
          Extract the name of places in the following text. 

          Desired format:
          Place: &lt;comma_separated_list_of_places&gt;
          
          Input: "Although these developments are encouraging to researchers, much is still a mystery. 
          “We often have a black box between the brain and the effect we see in the periphery,” says 
          Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. 
          “If we want to use it in the therapeutic context, we actually need to understand the mechanism.“"
        </code></pre>
      </div>

      <p>
        Output: <code>Place: Champalimaud Centre for the Unknown, Lisbon</code>
      </p>

      <p>
        It's often better to be specific and direct. The more direct, the more
        effective the message gets across. For example, you might be interested
        in learning the concept of prompt engineering. You might try something
        like:
      </p>
      <div class="responsive-code-container">
        <pre><code>
          ### Instruction ###
          Explain the concept prompt engineering. 
          Keep the explanation short, only a few sentences, and don't be too descriptive.
        </code></pre>
      </div>
      <p>
        It's not clear from the prompt above how many sentences to use and what
        style. You might still somewhat get good responses with the above
        prompts but the better prompt would be one that is very specific,
        concise, and to the point. Something like:
      </p>
      <div class="responsive-code-container">
        <pre><code>
          Use 2-3 sentences to explain the concept of prompt engineering to a high school student.
        </code></pre>
      </div>
      <p>
        Another common tip when designing prompts is to avoid saying what not to
        do but say what to do instead. This encourages more specificity and
        focuses on the details that lead to good responses from the model.
      </p>

      <p>
        Sources:
        <a
          href="https://www.promptingguide.ai/introduction/basics"
          target="_blank"
          >https://www.promptingguide.ai/introduction/basics</a
        >
      </p>

      <h4>2.2 Explain in-context learning.</h4>

      <p>
        In-context learning is a capability of large language models (LMs) to
        perform new tasks by conditioning on a few input-label pairs, known as
        demonstrations, during inference without any gradient updates or model
        retraining. This approach allows models to generalize to new tasks by
        observing task examples within the input context, effectively learning
        through inference alone
      </p>
      <p>Key Concepts of In-Context Learning:</p>
      <ul>
        <li>
          Demonstrations: The model is presented with examples of input-label
          pairs (e.g., question-answer pairs) that illustrate the task. These
          are concatenated in the input prompt along with a test example for
          which the model generates a prediction.
        </li>
        <li>
          Zero-shot vs. Few-shot: In-context learning enhances zero-shot
          methods, enabling models to perform tasks without prior exposure.
          However, few-shot learning, which involves presenting the model with a
          small set of task examples, typically leads to significantly higher
          accuracy and generalization across diverse tasks by offering
          context-specific guidance (but requires explicit training/fine-tuning
          phase).
        </li>
      </ul>
      <p>
        A fascinating insight from seminal research (<a
          href="https://arxiv.org/abs/2202.12837"
          target="_blank"
          >Min et al., 2022</a
        >) challenges our understanding of how language models learn from
        examples: the accuracy of labels in demonstrations appears to have
        minimal impact on task performance. Their experiments showed that even
        when correct labels are randomly replaced with incorrect ones, model
        performance remains largely unchanged across various classification and
        multiple-choice tasks. This counter-intuitive finding was consistently
        observed across different model scales, including GPT-3.
      </p>

      <p>
        Through extensive experimentation, the researchers uncovered that large
        language models heavily rely on superficial patterns rather than deep
        semantic understanding. This suggests that in-context learning functions
        more as sophisticated pattern matching than true learning - the model
        uses provided input-output examples to retrieve and apply similar
        patterns from its training data. However, this mechanism proves fragile:
        even minor modifications to labeling formats or demonstration templates
        can significantly degrade performance, revealing the brittle nature of
        this capability.
      </p>
      <h4>2.3 Explain types of prompt engineering.</h4>
      <p>
        Prompt engineering is the process of crafting and refining prompts to
        optimize the performance and accuracy of AI language models.
      </p>
      <ul>
        <li>
          <strong>Zero-shot Prompting</strong>
          <p>
            Description: This involves providing the model with a task or
            question without giving any examples.
          </p>
          <p>
            Use Case: When the model is expected to generalize from its
            pre-existing knowledge.
          </p>
          <p>Example: "Write a summary of the following text."</p>
        </li>
        <li>
          <strong>One-shot Prompting</strong>
          <p>
            Description: The model is given one example before performing the
            task.
          </p>
          <p>
            Use Case: Helps guide the model by offering a single reference
            point.
          </p>
          <p>
            Example: "Translate the following sentence into French. Example:
            'Hello' -> 'Bonjour'. Now translate: 'Good morning'."
          </p>
        </li>
        <li>
          <strong>Few-shot Prompting</strong>
          <p>
            Description: The model receives multiple examples to learn the
            pattern before completing the task.
          </p>
          <p>
            Use Case: Useful for complex tasks requiring nuanced understanding.
          </p>
          <p>
            Example: "Convert these active sentences to passive voice: 'John
            eats an apple.' -> 'An apple is eaten by John.' 'Sarah writes a
            book.' -> 'A book is written by Sarah.' Now convert: 'Tom kicks the
            ball.'"
          </p>
        </li>
        <li>
          <strong>Chain-of-thought Prompting</strong>
          <p>
            Description: Encourages the model to explain its reasoning process
            step by step.
          </p>
          <p>
            Use Case: Useful for problem-solving and logical reasoning tasks.
          </p>
          <p>
            Example: "Solve the following math problem by explaining each step.
            What is 45 divided by 3?"
          </p>
        </li>
        <li>
          <strong>Instruction-based Prompting</strong>
          <p>
            Description: The model is explicitly instructed to perform a task in
            a specific way.
          </p>
          <p>
            Use Case: Ensures the output follows a strict format or guideline.
          </p>
          <p>Example: "List three benefits of exercise in bullet points."</p>
        </li>
        <li>
          <strong>Persona-based Prompting</strong>
          <p>
            Description: The model is prompted to respond as if it were a
            specific character or role.
          </p>
          <p>
            Use Case: Enhances engagement or aligns the response with a
            particular tone or expertise.
          </p>
          <p>
            Example: "You are a nutritionist. Explain the benefits of a balanced
            diet."
          </p>
        </li>
        <li>
          <strong>Contextual Prompting</strong>
          <p>
            Description: The model is provided with relevant context or
            background information before answering.
          </p>
          <p>Use Case: Helps improve relevance and coherence of responses.</p>
          <p>
            Example: "Given that climate change is accelerating, suggest three
            ways to reduce carbon emissions."
          </p>
        </li>
        <li>
          <strong>Iterative Prompting</strong>
          <p>
            Description: The prompt evolves based on feedback or prior outputs.
          </p>
          <p>Use Case: Improves performance through a refinement loop.</p>
          <p>
            Example: "Rewrite the following paragraph to make it clearer. If
            necessary, suggest additional edits."
          </p>
        </li>
      </ul>
      <h4>
        2.4 What are some aspects to keep in mind while using few-shots
        prompting?
      </h4>
      <p>
        <strong>Few-shot prompting</strong> has emerged as a powerful technique
        in natural language processing, particularly valuable when labeled data
        is scarce. This approach, which involves providing a model with
        carefully selected examples to guide its responses, requires thoughtful
        consideration of several key elements to maximize its effectiveness. At
        the foundation of successful few-shot prompting lies the art of
        <strong>example selection</strong> and <strong>formatting</strong>. The
        examples you choose should represent a diverse range of scenarios within
        your task's scope while maintaining clarity and unambiguity. It's
        crucial to establish a consistent format across all examples, clearly
        delineating inputs from outputs to help the model understand the pattern
        you want it to follow.
      </p>

      <p>
        The question of <strong>quantity</strong> often arises in few-shot
        prompting. While there's no universal rule, experience shows that three
        to five examples typically provide a sweet spot between sufficient
        context and avoiding cognitive overload. This number can be adjusted
        based on your specific task's complexity and requirements. The
        <strong>arrangement</strong> of these examples matters significantly -
        starting with simpler cases and progressively moving to more complex
        ones helps the model build understanding gradually, much like how humans
        learn new concepts.
      </p>

      <p>
        <strong>Task clarity</strong> plays a vital role in the success of
        few-shot prompting. Before presenting any examples, it's essential to
        establish a clear definition of the task or objective. For complex
        tasks, explicit instructions can serve as valuable guardrails, helping
        the model stay on track and deliver more accurate results.
      </p>

      <p>
        The <strong>sensitivity</strong> of language models to subtle variations
        in prompting cannot be overstated. Minor changes in phrasing or example
        order can significantly impact performance. This characteristic makes it
        essential to experiment with different approaches and validate outputs
        regularly. Whether through manual review, automated testing, or
        cross-validation, consistent <strong>output verification</strong> helps
        ensure the model maintains alignment with expected outcomes.
      </p>

      <p>
        <strong>Domain adaptation</strong> represents another crucial aspect of
        effective few-shot prompting. The examples you provide should reflect
        the specific context and terminology of your target domain. This
        alignment between examples and domain context significantly enhances the
        relevance and accuracy of the model's outputs, leading to more practical
        and applicable results.
      </p>
      <h4>2.5 What are certain strategies to write good prompts?</h4>

      <p>
        Craft effective prompts to elicit clear, relevant, and creative
        responses. Be specific to avoid vagueness.
      </p>
      <ul>
        <li>
          Use "Describe a smartphone for seniors with usability features"
          instead of "Describe a product."
        </li>
        <li>
          Provide context, e.g., "You are a marketing manager launching an
          energy drink. Write an ad for college students."
        </li>
        <li>
          Ask open-ended questions, like "What are the benefits and challenges
          of remote work?" to encourage detailed answers.
        </li>
        <li>
          Guide responses with examples and constraints, such as "Summarize a
          novel in 100 words with a plot twist."
        </li>
        <li>
          Specify tone and style for the audience: "Write a persuasive letter
          advocating for public transportation expansion."
        </li>
        <li>
          Break down complex tasks: "List three waste-reduction strategies and
          explain their implementation."
        </li>
        <li>
          Test and refine prompts by adjusting phrasing. Clarify by adding
          details, e.g., "Describe a futuristic city in 2050 focusing on
          technology and sustainability."
        </li>
      </ul>

      <h4>
        2.6 What is hallucination, and how can it be controlled using prompt
        engineering?
      </h4>
      <p>
        Hallucination in AI refers to the generation of incorrect, nonsensical,
        or fabricated outputs by models, particularly in NLP and generative
        tasks. These outputs are not grounded in data but appear plausible. This
        occurs when models misinterpret input or overgeneralize patterns.
      </p>
      <p>
        Manifestations include:
        <strong>Factual inaccuracies</strong> (False information),
        <strong>Logical inconsistencies</strong> (Contradictions or errors), and
        <strong>Fabrication</strong> (Nonexistent references or data).
      </p>
      <p>
        Causes of hallucination include incomplete or biased training data, the
        complexity of models that generate diverse but occasionally erroneous
        outputs, vague or ambiguous input that prompts fabrication, and
        overfitting, where models memorize data instead of effectively
        generalizing.
      </p>
      <p>
        Prompt engineering refines input to guide AI toward accurate outputs,
        minimizing hallucinations.
      </p>
      <p>Key Techniques to optimize outputs:</p>
      <ul>
        <li>
          <strong>Precise Language:</strong> Use specific prompts. For example,
          instead of a vague prompt, say, "List NASA space milestones from 1960
          to 2020." This helps the model understand exactly what information you
          are seeking.
        </li>
        <li>
          <strong>Context:</strong> Add relevant background. For instance, if
          you want a summary of a report, say, "Summarize the 2022 IPCC climate
          report." Providing context helps the model generate more accurate and
          relevant information.
        </li>
        <li>
          <strong>Scope Limitation:</strong> Limit the scope of the response.
          For example, ask, "Give five facts about renewable energy, excluding
          fossil fuels." This focuses the model's output and avoids unnecessary
          information.
        </li>
        <li>
          <strong>Break Down Queries:</strong> Enhance clarity by breaking down
          queries into parts. For example, ask, "Explain the greenhouse effect,
          then describe CO2's role." This step-by-step approach guides the model
          through complex topics more effectively.
        </li>
        <li>
          <strong>Request Sources:</strong> Add credibility by requesting
          sources or citations. For example, say, "List three studies on
          meditation reducing stress." This encourages the model to provide
          verifiable information.
        </li>
      </ul>
      <p>
        Regular monitoring, feedback, and iterative prompt refinement reduce
        hallucinations. In critical tasks, human oversight ensures accuracy.
      </p>
      <h4>
        2.7 How to improve the reasoning ability of LLM through prompt
        engineering?
      </h4>
      <p>
        Improving the reasoning ability of large language models (LLMs) through
        prompt engineering involves using specific strategies to guide the
        model's thought process. Here are some effective techniques:
      </p>
      <ol>
        <li>
          <strong>Chain-of-Thought Prompting:</strong> Enable step-by-step
          reasoning by explicitly asking the model to break down its thinking
          process. For example, instead of "What's 13 x 27?", use "Let's solve
          13 x 27 step by step. Think through each part of the calculation."
          Chain-of-thought (CoT) prompting, as originally outlined by Wei et.
          al. in their paper
          <a href="https://arxiv.org/abs/2201.11903" target="_blank"
            >"Chain of Thought Prompting Elicits Reasoning in Large Language
            Models"</a
          >, is a form of few-shot prompting. Few-shot prompting, in contrast to
          zero-shot prompting, involves providing an “exemplar” (example) of a
          similar question and answer pair before posing the question which the
          LLM should solve. In CoT prompting, the exemplar answer walks through
          the problem step-by-step. This compels the LLM to then respond to the
          real question step-by-step, mimicking the exemplar answer.
        </li>
        <li>
          <strong>Zero-Shot Chain-of-Thought:</strong> Zero-shot
          chain-of-thought prompting, as outlined by Kojima et. al. in their
          paper
          <a href="https://arxiv.org/abs/2205.11916" target="_blank"
            >"Large Language Models are Zero-Shot Reasoners"</a
          >, attempts to emulate this effect by simply appending the phrase
          “let’s think step by step” to the prompt, without providing an
          exemplar. It is crucial to understand that this zero-shot
          chain-of-thought prompting is the form of CoT prompting which this
          novel method seeks to improve, not the original “few-shot” CoT
          prompting.
        </li>
        <img
          src="../assets/cot.webp"
          alt="Visualization of CoR and LoT prompting"
          class="responsive-img"
          width="720"
          height="480"
        />
        <li>
          <strong>LoT Prompting (Logic of Thought Prompting):</strong>

          LoT prompting begins by asking the LLM to solve the problem step by
          step, similar to zero-shot chain-of-thought prompting. After the LLM
          provides its initial step-by-step solution, follow-up prompts instruct
          it to verify each step. Specifically, the LLM is asked to give both a
          positive and negative review for each step, then justify the correct
          part of the reasoning and criticize the incorrect one, based on the
          original problem’s premises. If necessary, the LLM is directed to
          revise the step according to its own critique. Once a step is revised
          or confirmed, the original problem is restated, along with the
          verified or updated steps, to ensure the solution is coherent and
          accurate.
        </li>
        <br />
        <img
          src="../assets/LoT.webp"
          alt="LoT mechanism"
          class="responsive-img"
          width="720"
          height="480"
        />
        <li>
          <strong>Self-Consistency:</strong> Ask the model to approach the
          problem from multiple angles and cross-check its reasoning. For
          instance, "Solve this problem using two different methods and verify
          that they give the same result."
        </li>
        <li>
          <strong>Program of Thoughts (PoT):</strong>
          Program of Thoughts (PoT) is a prompting technique that combines the
          strengths of Chain-of-Thought (CoT) and program synthesis. It involves
          generating a program or a sequence of instructions that the model can
          follow to solve a problem. This approach leverages the model's ability
          to understand and execute structured instructions, leading to more
          accurate and reliable reasoning. In PoT, the large language models
          (LLMs) are primarily responsible for expressing the ‘reasoning
          process’ in a programming language, while the computation is delegated
          to an external process, such as a Python interpreter. By disentangling
          computation from reasoning, PoT addresses challenges LLMs face in
          generating complex equations, such as those involving cubes or
          polynomials. Key aspects of PoT include:
          <ol>
            <li>Breaking down problems into a multi-step ‘thought’ process.</li>
            <li>
              Binding semantic meanings to variables. By using semantically
              meaningful variable names (e.g., <code>interest_rate</code> or
              <code>sum_in_two_years_with_interest</code>), PoT creates a
              structured representation that aligns with human reasoning.
            </li>
          </ol>
          This structured approach enhances the model's ability to solve tasks
          involving numerical reasoning and logical inference.

          <p>PoT Evaluation:</p>
          <ul>
            <li>
              PoT has been tested across multiple datasets, including
              mathematical word problems (MWP) and financial datasets.
            </li>
            <li>
              In few-shot settings, PoT showed an average gain of 8% over CoT on
              MWP datasets and 15% on financial datasets. In zero-shot settings,
              it achieved a 12% average gain on MWP datasets.
            </li>
            <li>
              Combining PoT with self-consistency (SC) decoding further improved
              performance, outperforming CoT+SC by an average of 10%.
            </li>
            <li>
              PoT+SC achieved the best-known results on MWP datasets and
              near-best results on financial datasets (excluding GPT-4).
            </li>
          </ul>

          <p>
            The following diagram illustrates how PoT resolves complex problems
            that traditional CoT approaches fail to address, utilizing Python
            programs to express reasoning and relying on a Python interpreter
            for computation.
          </p>
          <img
            src="../assets/pot.jpg"
            alt="PoT mechanism"
            class="responsive-img"
            width="720"
            height="480"
          />
          <p>
            Source:
            <a href="https://arxiv.org/pdf/2211.12588.pdf"
              >arxiv.org/2211.12588</a
            >
          </p>
        </li>

        <li>
          <strong>Structure Decomposition:</strong> Break complex problems into
          smaller sub-problems. For example, instead of "Analyze this investment
          strategy," use "Let's analyze this investment strategy by first
          examining the risk factors, then calculating potential returns, and
          finally evaluating market conditions."
        </li>
        <li>
          <strong>Explicit Constraints and Requirements:</strong> Clearly state
          what constitutes valid reasoning. For example, "Your solution must
          include clear assumptions, mathematical justification, and potential
          edge cases."
        </li>
        <li>
          <strong>Role Prompting:</strong> Assign a specific expert role to
          encourage domain-specific reasoning. For example, "As a mathematical
          logician, explain why..."
        </li>
        <li>
          <strong>Meta-Cognition Prompting:</strong> Ask the model to evaluate
          its own reasoning. For example, "After providing your solution,
          explain which parts of your reasoning you're most and least confident
          about."
        </li>
      </ol>
      <h4>2.8 How to improve LLM reasoning if your COT prompt fails?</h4>
      <p>
        CoT prompts help guide logical steps, but their effectiveness depends
        heavily on their design and context.
      </p>
      <p>
        <strong>Clarity</strong> in the prompt is crucial. Specific instructions
        tailored to the problem often yield better results than vague requests.
        Breaking problems into smaller parts makes reasoning more manageable,
        particularly when combined with illustrative examples. For instance,
        guiding the model to identify known quantities, apply formulas, and
        simplify step-by-step enhances accuracy.
      </p>
      <p>
        <strong>Context</strong> is another key factor. Prompts lacking
        sufficient information can confuse the model. Including necessary
        background or providing structured inputs such as diagrams or tables
        helps clarify expectations. <strong>Rephrasing</strong> questions or
        representing problems visually can also aid reasoning. Experimenting
        with different phrasings often reveals what works best for the model.
        <strong> Self-verification </strong> is a valuable technique. Prompts
        that ask the model to review and justify its solutions can uncover
        errors and fill gaps in logic. Integrating
        <strong> external tools </strong>can further support structured
        reasoning. For example, pairing the LLM with symbolic solvers or
        computation platforms like WolframAlpha ensures precision in tasks
        requiring formal logic or accurate calculations.
      </p>
      <h2 id="rag">3. Retrieval Augmented Generation (RAG)</h2>
      <h4>
        3.1 How to increase accuracy, reliability, and make answers verifiable
        in LLM?
      </h4>
      <h4>3.2 How does RAG work?</h4>
      <h4>3.3 What are some benefits of using the RAG system?</h4>
      <h4>3.4 When should I use Fine-tuning instead of RAG?</h4>
      <h4>
        3.5 What are the architecture patterns for customizing LLM with
        proprietary data?
      </h4>

      <h2 id="fine-tuning">4. Fine-tuning of LLMs</h2>

      <h4>4.1 What is fine-tuning, and why is it needed?</h4>
      <h4>4.2 Which scenario do we need to fine-tune LLM?</h4>
      <h4>4.3 How to make the decision of fine-tuning?</h4>
      <h4>
        4.4 How do you improve the model to answer only if there is sufficient
        context for doing so?
      </h4>
      <h4>4.5 How to create fine-tuning datasets for Q&A?</h4>
      <h4>4.6 How to set hyperparameters for fine-tuning?</h4>
      <h4>
        4.7 How to estimate infrastructure requirements for fine-tuning LLM?
      </h4>
      <h4>4.8 How do you fine-tune LLM on consumer hardware?</h4>
      <h4>4.9 What are the different categories of the PEFT method?</h4>
      <h4>4.10 What is catastrophic forgetting in LLMs?</h4>
      <h4>4.11 What are different re-parameterized methods for fine-tuning?</h4>

      <h2 id="agent-based">5. Agent-Based System</h2>
      <h4>
        5.1 Explain the basic concepts of an agent and the types of strategies
        available to implement agents.
      </h4>
      <h4>
        5.2 Why do we need agents and what are some common strategies to
        implement agents?
      </h4>
      <h4>
        5.3 Explain ReAct prompting with a code example and its advantages.
      </h4>
      <h4>5.4 Explain Plan and Execute prompting strategy.</h4>
      <h4>5.5 Explain OpenAI functions strategy with code examples.</h4>
      <h4>
        5.6 Explain the difference between OpenAI functions vs LangChain Agents.
      </h4>
      <h2 id="miscellaneous">6. Miscellaneous</h2>
      <h4>6.1 How to optimize the cost of the overall LLM system?</h4>
      <h4>6.2 What are mixture of expert models (MoE)?</h4>
      <h4>
        6.3 How to build a production-grade RAG system, explain each component
        in detail?
      </h4>
      <h4>6.4 What is FP8 variable and what are its advantages?</h4>
      <h4>
        6.5 How to train LLM with low precision training without compromising on
        accuracy?
      </h4>
      <h4>6.6 How to calculate the size of the KV cache?</h4>
      <h4>
        6.7 Explain the dimension of each layer in a multi-headed transformation
        attention block.
      </h4>
      <h4>
        6.8 How do you make sure that the attention layer focuses on the right
        part of the input?
      </h4>
    </div>

    <script>
      // Get stored dark mode state
      let isDarkMode = localStorage.getItem("darkMode") === "true";

      // Function to apply dark mode
      function applyDarkMode(dark) {
        const darkModeToggle = document.getElementById("darkModeToggle");
        const elements = [
          document.body,
          ...document.querySelectorAll(
            ".main-container, .info-container, .projects-container, .article-container, .link-container, .accordion, .code"
          ),
        ];

        // Update state
        isDarkMode = dark;
        localStorage.setItem("darkMode", dark);

        // Update UI
        elements.forEach((element) => {
          if (dark) {
            element.classList.add("dark-mode");
          } else {
            element.classList.remove("dark-mode");
          }
        });

        // Update toggle
        if (darkModeToggle) {
          darkModeToggle.checked = dark;
        }
      }

      // Initialize dark mode
      document.addEventListener("DOMContentLoaded", () => {
        const darkModeToggle = document.getElementById("darkModeToggle");

        // Apply initial state
        applyDarkMode(isDarkMode);

        // Handle toggle changes
        darkModeToggle.addEventListener("change", (e) => {
          applyDarkMode(e.target.checked);
        });
      });

      // Apply dark mode immediately if needed
      if (isDarkMode) {
        applyDarkMode(true);
      }
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  </body>
</html>
