<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="A comprehensive guide to understanding linear and logistic regression in machine learning, with practical Python implementations and detailed comparisons."
    />

    <meta
      name="keywords"
      content="linear regression, logistic regression, machine learning, python, data science, statistics, classification, prediction"
    />
    <meta name="author" content="Francesco Sannicola" />
    <meta
      property="og:title"
      content="Francesco Sannicola - Machine Learning Engineer"
    />
    <meta
      property="og:description"
      content="Understanding Linear and Logistic Regression: A comprehensive deep dive with Python implementations and practical examples."
    />
    <meta
      property="og:url"
      content="https://www.francescosannicola.com/articles/linear-logistic-regression"
    />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="./assets/francescosannicola.jpg" />
    <title>
      Linear and Logistic Regression: A Deep Dive - Francesco Sannicola
    </title>
    <link rel="icon" type="image/x-icon" href="../favicon.ico" />
    <script
      type="text/javascript"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="stylesheet" type="text/css" href="../style.css" />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="main-container">
      <div class="left-column">
        <header>
          <div class="header-left">
            <h1>Francesco Sannicola</h1>
            <h4 class="role">Machine Learning | Software Engineering</h4>
          </div>

          <div class="header-right">
            <div class="contact-info">
              <a href="../resume.pdf" target="_blank" aria-label="View Resume">
                <svg
                  fill="#000000"
                  version="1.1"
                  id="Capa_1"
                  xmlns="http://www.w3.org/2000/svg"
                  xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="18"
                  height="18"
                  viewBox="0 0 45.057 45.057"
                  xml:space="preserve"
                >
                  <title>Resume</title>
                  <g>
                    <g id="_x35_8_24_">
                      <g>
                        <path
                          d="M19.558,25.389c-0.067,0.176-0.155,0.328-0.264,0.455c-0.108,0.129-0.24,0.229-0.396,0.301
                                       c-0.156,0.072-0.347,0.107-0.57,0.107c-0.313,0-0.572-0.068-0.78-0.203c-0.208-0.137-0.374-0.316-0.498-0.541
                                       c-0.124-0.223-0.214-0.477-0.27-0.756c-0.057-0.279-0.084-0.564-0.084-0.852c0-0.289,0.027-0.572,0.084-0.853
                                       c0.056-0.281,0.146-0.533,0.27-0.756c0.124-0.225,0.29-0.404,0.498-0.541c0.208-0.137,0.468-0.203,0.78-0.203
                                       c0.271,0,0.494,0.051,0.666,0.154c0.172,0.105,0.31,0.225,0.414,0.361c0.104,0.137,0.176,0.273,0.216,0.414
                                       c0.04,0.139,0.068,0.25,0.084,0.33h2.568c-0.112-1.08-0.49-1.914-1.135-2.502c-0.644-0.588-1.558-0.887-2.741-0.895
                                       c-0.664,0-1.263,0.107-1.794,0.324c-0.532,0.215-0.988,0.52-1.368,0.912c-0.38,0.392-0.672,0.863-0.876,1.416
                                       c-0.204,0.551-0.307,1.165-0.307,1.836c0,0.631,0.097,1.223,0.288,1.77c0.192,0.549,0.475,1.021,0.847,1.422
                                       s0.825,0.717,1.361,0.949c0.536,0.23,1.152,0.348,1.849,0.348c0.624,0,1.18-0.105,1.668-0.312
                                       c0.487-0.209,0.897-0.482,1.229-0.822s0.584-0.723,0.756-1.146c0.172-0.422,0.259-0.852,0.259-1.283h-2.593
                                       C19.68,25.023,19.627,25.214,19.558,25.389z"
                        />
                        <polygon
                          points="26.62,24.812 26.596,24.812 25.192,19.616 22.528,19.616 25.084,28.184 28.036,28.184 30.713,19.616 28,19.616"
                        />
                        <path
                          d="M33.431,0H5.179v45.057h34.699V6.251L33.431,0z M36.878,42.056H8.179V3h23.706v4.76h4.992L36.878,42.056L36.878,42.056z"
                        />
                      </g>
                    </g>
                  </g>
                </svg>
                <span class="contact-label">Resume</span>
              </a>

              <a
                href="mailto:francescosannicola1997@gmail.com"
                aria-label="Send Email"
              >
                <svg
                  width="18"
                  height="21"
                  fill="#000000"
                  viewBox="0 0 1920 1920"
                  xmlns="http://www.w3.org/2000/svg"
                >
                  <title>Email</title>
                  <path
                    d="M0 1694.235h1920V226H0v1468.235ZM112.941 376.664V338.94H1807.06v37.723L960 1111.233l-847.059-734.57ZM1807.06 526.198v950.513l-351.134-438.89-88.32 70.475 378.353 472.998H174.042l378.353-472.998-88.32-70.475-351.134 438.89V526.198L960 1260.768l847.059-734.57Z"
                    fill-rule="evenodd"
                  />
                </svg>
                <span class="contact-label">Email</span>
              </a>

              <a
                href="https://www.linkedin.com/in/francesco-sannicola"
                target="_blank"
                aria-label="View LinkedIn Profile"
              >
                <svg
                  fill="#000000"
                  width="18"
                  height="18"
                  viewBox="0 0 1920 1920"
                  xmlns="http://www.w3.org/2000/svg"
                >
                  <title>LinkedIn</title>
                  <path
                    d="M1168 601.321v74.955c72.312-44.925 155.796-71.11 282.643-71.11 412.852 0 465.705 308.588 465.705 577.417v733.213L1438.991 1920v-701.261c0-117.718-42.162-140.06-120.12-140.06-74.114 0-120.12 23.423-120.12 140.06V1920l-483.604-4.204V601.32H1168Zm-687.52-.792v1318.918H0V600.53h480.48Zm-120.12 120.12H120.12v1078.678h240.24V720.65Zm687.52.792H835.267v1075.316l243.364 2.162v-580.18c0-226.427 150.51-260.18 240.24-260.18 109.55 0 240.24 45.165 240.24 260.18v580.18l237.117-2.162v-614.174c0-333.334-93.573-457.298-345.585-457.298-151.472 0-217.057 44.925-281.322 98.98l-16.696 14.173H1047.88V721.441ZM240.24 0c132.493 0 240.24 107.748 240.24 240.24 0 132.493-107.747 240.24-240.24 240.24C107.748 480.48 0 372.733 0 240.24 0 107.748 107.748 0 240.24 0Zm0 120.12c-66.186 0-120.12 53.934-120.12 120.12s53.934 120.12 120.12 120.12 120.12-53.934 120.12-120.12-53.934-120.12-120.12-120.12Z"
                    fill-rule="evenodd"
                  />
                </svg>
                <span class="contact-label">LinkedIn</span>
              </a>

              <a
                href="https://github.com/francesco-s"
                target="_blank"
                aria-label="View GitHub Profile"
              >
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  x="0px"
                  y="0px"
                  width="100"
                  height="100"
                  viewBox="0 0 24 24"
                >
                  <title>GitHub</title>
                  <path
                    d="M 12 1.9921875 C 6.4855957 1.9921875 2 6.4769321 2 12 C 2 16.599161 5.1205653 20.490345 9.3671875 21.642578 A 0.50005 0.50005 0 0 0 9.9980469 21.160156 L 9.9980469 17.583984 A 0.50005 0.50005 0 0 0 9.9980469 17.398438 L 9.9980469 17 C 9.9980469 16.463435 10.20474 15.989016 10.550781 15.621094 A 0.50005 0.50005 0 0 0 10.310547 14.794922 C 8.3191985 14.288505 7 12.945349 7 11.492188 C 7 10.73278 7.3615845 9.9960348 8.015625 9.3925781 A 0.50005 0.50005 0 0 0 8.1679688 8.9277344 C 8.066665 8.41934 8.082645 7.8782734 8.0742188 7.34375 C 8.6127591 7.5825374 9.1625862 7.7923277 9.640625 8.1757812 A 0.50005 0.50005 0 0 0 10.09375 8.265625 C 10.683558 8.0925899 11.325263 7.9921875 12 7.9921875 C 12.673669 7.9921875 13.314937 8.0917915 13.904297 8.265625 A 0.50005 0.50005 0 0 0 14.359375 8.1777344 C 14.837054 7.7945694 15.386932 7.5824899 15.925781 7.34375 C 15.916981 7.8785794 15.933881 8.4199143 15.832031 8.9277344 A 0.50005 0.50005 0 0 0 15.982422 9.3945312 C 16.637666 9.9982795 17.001953 10.733923 17.001953 11.494141 C 17.001953 12.945944 15.680991 14.28753 13.689453 14.794922 A 0.50005 0.50005 0 0 0 13.449219 15.623047 C 13.794633 15.990353 14.001953 16.464434 14.001953 17 L 13.998047 21.160156 A 0.50005 0.50005 0 0 0 14.630859 21.642578 C 18.878428 20.490372 22 16.599237 22 12 C 22 6.4769321 17.514404 1.9921875 12 1.9921875 z M 12 2.9921875 C 16.973596 2.9921875 21 7.0170679 21 12 C 21 15.899732 18.470363 19.145081 14.998047 20.388672 L 15.001953 17 C 15.001953 16.421513 14.688987 15.964437 14.404297 15.505859 C 16.432596 14.813419 18.001953 13.369173 18.001953 11.494141 C 18.001953 10.506223 17.528746 9.6094427 16.826172 8.8808594 C 16.963004 8.0965535 17.032667 7.2949738 16.970703 6.4570312 A 0.50005 0.50005 0 0 0 16.3125 6.0214844 C 15.491527 6.300863 14.703446 6.7215379 13.960938 7.2734375 C 13.34208 7.110241 12.692308 6.9921875 12 6.9921875 C 11.30618 6.9921875 10.656139 7.1108109 10.037109 7.2734375 C 9.2949426 6.7217093 8.5094793 6.2989123 7.6894531 6.0195312 A 0.50005 0.50005 0 0 0 7.0292969 6.4550781 C 6.9654142 7.294134 7.0380099 8.0939393 7.1738281 8.8769531 C 6.4709034 9.6061723 6 10.504427 6 11.492188 C 6 13.367279 7.5663139 14.813867 9.59375 15.505859 C 9.3112432 15.962509 9.0029789 16.419068 9 16.992188 L 7.5 16.992188 C 7.2241522 16.992188 7.0052524 16.845238 6.7363281 16.513672 C 6.4674039 16.182105 6.2065255 15.697988 5.9277344 15.234375 A 0.50005 0.50005 0 0 0 5.46875 14.988281 A 0.50005 0.50005 0 0 0 5.0722656 15.75 C 5.3304745 16.179387 5.5979086 16.697395 5.9589844 17.142578 C 6.3200601 17.587762 6.8328478 17.992188 7.5 17.992188 L 8.9980469 17.992188 L 8.9980469 20.388672 C 5.5278881 19.144471 3 15.898951 3 12 C 3 7.0170679 7.0264043 2.9921875 12 2.9921875 z"
                  ></path>
                </svg>
                <span class="contact-label">GitHub</span>
              </a>
            </div>
          </div>
          <div class="switch">
            <input type="checkbox" id="darkModeToggle" />
            <label for="darkModeToggle" class="switch-label">
              <span class="sun-icon">&#9728;</span>
              <span class="moon-icon">&#9790;</span>
            </label>
          </div>
        </header>
      </div>
    </div>
    <div class="link-container">
      <div class="link-column">
        <a href="../"> <i class="fas fa-home"></i>Home </a>
      </div>
      <div class="link-column">
        <a href="../projects/">
          <i class="fas fa-project-diagram"></i>Projects
        </a>
      </div>
      <div class="link-column">
        <a href="../articles/"> <i class="fas fa-newspaper"></i>Articles </a>
      </div>
      <div class="link-column">
        <a href="../about.html"> <i class="fas fa-user"></i>About </a>
      </div>
    </div>
    <div class="article-container">
      <h1>
        <span style="font-size: 16px">./articles/</span> Understanding Linear
        and Logistic Regression: A Deep Dive and Comparison
      </h1>
      <div class="last-updated">
        <span class="last-updated-label">Last modified:</span>
        <time datetime="2025-01-22">January 22, 2025</time>
      </div>

      <p>
        Regression analysis is one of the fundamental techniques in machine
        learning and statistics, providing powerful tools for both prediction
        and understanding relationships between variables. Among the various
        regression methods, linear and logistic regression stand out as
        essential techniques that every data scientist and machine learning
        practitioner should master.
      </p>

      <p>
        While both methods share the word "regression" in their names, they
        serve distinctly different purposes and are built on different
        mathematical foundations. Linear regression is used for predicting
        continuous numerical values, while logistic regression is designed for
        classification tasks. Understanding when and how to use each method is
        crucial for successful machine learning projects.
      </p>

      <p>
        This comprehensive guide will take you through both techniques,
        exploring their mathematical foundations, practical implementations in
        Python, and real-world applications. By the end of this article, you'll
        have a clear understanding of when to use each method and how to
        implement them effectively.
      </p>

      <div class="summary">
        <h4>Quick Navigation</h4>
        <ol>
          <li><a href="#introduction">Introduction</a></li>
          <li><a href="#linear-regression">Linear Regression</a></li>
          <li><a href="#logistic-regression">Logistic Regression</a></li>
          <li>
            <a href="#key-differences"
              >Key Differences Between Linear and Logistic Regression</a
            >
          </li>
          <li><a href="#practical-examples">Practical Examples</a></li>
          <li><a href="#when-to-use">When to Use Which?</a></li>
          <li><a href="#conclusion">Conclusion</a></li>
        </ol>
      </div>

      <h2 id="introduction">1. Introduction</h2>

      <h4 class="accordion">
        1.1 Brief overview of regression in machine learning<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Regression analysis is a statistical method used to model the
          relationship between a dependent variable (target) and one or more
          independent variables (features). In machine learning, regression
          serves two primary purposes:
        </p>
        <ul>
          <li>
            <strong>Prediction:</strong> Estimating the value of an unknown
            variable based on known input features
          </li>
          <li>
            <strong>Inference:</strong> Understanding the relationships and
            dependencies between variables
          </li>
        </ul>
        <p>
          Regression techniques form the foundation of many advanced machine
          learning algorithms and are widely used across industries for tasks
          such as financial forecasting, medical diagnosis, marketing analytics,
          and engineering optimization.
        </p>
      </div>

      <h4 class="accordion">
        1.2 Importance of understanding the difference between linear and
        logistic regression<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Many beginners in machine learning confuse linear and logistic
          regression due to their similar names. However, they solve
          fundamentally different types of problems:
        </p>
        <ul>
          <li>
            <strong>Linear Regression:</strong> Predicts continuous numerical
            values (regression problems)
          </li>
          <li>
            <strong>Logistic Regression:</strong> Predicts discrete categories
            or probabilities (classification problems)
          </li>
        </ul>
        <p>Understanding this distinction is crucial for:</p>
        <ul>
          <li>Choosing the appropriate algorithm for your specific problem</li>
          <li>Interpreting model results correctly</li>
          <li>Selecting proper evaluation metrics</li>
          <li>Understanding the assumptions and limitations of each method</li>
        </ul>
      </div>

      <h4 class="accordion">
        1.3 Objectives of the article<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          This article aims to provide a comprehensive understanding of both
          linear and logistic regression by covering:
        </p>
        <ul>
          <li>Mathematical foundations and assumptions of each method</li>
          <li>Practical Python implementations with real examples</li>
          <li>Evaluation metrics and interpretation techniques</li>
          <li>Comparative analysis highlighting key differences</li>
          <li>Guidelines for choosing the appropriate method</li>
          <li>Common pitfalls and best practices</li>
        </ul>
      </div>

      <h2 id="linear-regression">2. Linear Regression</h2>

      <h4 class="accordion">
        2.1 What is linear regression?<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Linear regression is a statistical method that models the relationship
          between a dependent variable and one or more independent variables
          using a linear equation. The goal is to find the best-fitting straight
          line (or hyperplane in multiple dimensions) through the data points.
        </p>
        <p>
          The fundamental assumption is that the relationship between the input
          features and the target variable can be approximated by a linear
          function. This makes linear regression both interpretable and
          computationally efficient.
        </p>
      </div>

      <h4 class="accordion">
        2.2 Mathematical formulation<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          For simple linear regression with one independent variable, the model
          is expressed as:
        </p>
        <p class="responsive-math">\[ y = \beta_0 + \beta_1 x + \epsilon \]</p>
        <p>Where:</p>
        <ul>
          <li>\( y \) = dependent variable (target)</li>
          <li>\( x \) = independent variable (feature)</li>
          <li>\( \beta_0 \) = y-intercept (bias term)</li>
          <li>\( \beta_1 \) = slope (coefficient)</li>
          <li>\( \epsilon \) = error term</li>
        </ul>

        <p>For multiple linear regression with multiple features:</p>
        <p class="responsive-math">
          \[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n +
          \epsilon \]
        </p>

        <p>In matrix form:</p>
        <p class="responsive-math">
          \[ \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
          \]
        </p>

        <p>
          The coefficients are typically estimated using the method of least
          squares:
        </p>
        <p class="responsive-math">
          \[ \boldsymbol{\hat{\beta}} =
          (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \]
        </p>
      </div>

      <h4 class="accordion">
        2.3 Assumptions of linear regression<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>Linear regression relies on several key assumptions:</p>
        <ul>
          <li>
            <strong>Linearity:</strong> The relationship between features and
            target is linear
          </li>
          <li>
            <strong>Independence:</strong> Observations are independent of each
            other
          </li>
          <li>
            <strong>Homoscedasticity:</strong> Constant variance of residuals
          </li>
          <li>
            <strong>Normality:</strong> Residuals are normally distributed
          </li>
          <li>
            <strong>No multicollinearity:</strong> Features are not highly
            correlated with each other
          </li>
        </ul>
        <p>
          Violating these assumptions can lead to biased estimates, incorrect
          confidence intervals, and poor predictive performance.
        </p>
      </div>

      <h4 class="accordion">
        2.4 Use cases and evaluation metrics<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p><strong>Common use cases:</strong></p>
        <ul>
          <li>
            Predicting house prices based on features like size, location, and
            age
          </li>
          <li>Forecasting sales revenue based on marketing spend</li>
          <li>Estimating stock prices based on financial indicators</li>
          <li>Predicting temperature based on historical weather data</li>
        </ul>

        <p><strong>Key evaluation metrics:</strong></p>
        <ul>
          <li>
            <strong>Mean Squared Error (MSE):</strong> \( MSE =
            \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2 \)
          </li>
          <li>
            <strong>Root Mean Squared Error (RMSE):</strong> \( RMSE =
            \sqrt{MSE} \)
          </li>
          <li>
            <strong>R-squared (R²):</strong> \( R^2 = 1 -
            \frac{SS_{res}}{SS_{tot}} \)
          </li>
          <li>
            <strong>Mean Absolute Error (MAE):</strong> \( MAE =
            \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i| \)
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        2.5 Python Implementation<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Here's a complete implementation of linear regression using
          scikit-learn:
        </p>

        <div class="responsive-code-container">
          <pre><code class="language-python">
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Example data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([1.5, 3.5, 4.5, 6.5, 8])

# Fit model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Metrics
print("MSE:", mean_squared_error(y, y_pred))
print("R^2:", r2_score(y, y_pred))
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Visualization
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X, y_pred, color='red', label='Fitted line')
plt.title("Linear Regression Example")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.grid(True)
plt.show()
          </code></pre>
        </div>
      </div>

      <h2 id="logistic-regression">3. Logistic Regression</h2>

      <h4 class="accordion">
        3.1 What is logistic regression?<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Logistic regression is a statistical method used for binary and
          multiclass classification problems. Despite its name containing
          "regression," it's actually a classification algorithm that predicts
          the probability of an instance belonging to a particular category.
        </p>
        <p>
          Unlike linear regression, which predicts continuous values, logistic
          regression uses the logistic (sigmoid) function to map any real-valued
          input to a value between 0 and 1, making it suitable for probability
          estimation and classification.
        </p>
      </div>

      <h4 class="accordion">
        3.2 Mathematical formulation (sigmoid function, log-odds)<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          The logistic regression model uses the sigmoid function to transform
          linear combinations of features into probabilities:
        </p>
        <p class="responsive-math">
          \[ p(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n
          x_n)}} \]
        </p>

        <p>The sigmoid function can be written as:</p>
        <p class="responsive-math">\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]</p>

        <p>
          Where \( z = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n \) is the
          linear combination.
        </p>

        <p>The log-odds (logit) transformation is:</p>
        <p class="responsive-math">
          \[ \ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + ... +
          \beta_n x_n \]
        </p>

        <p>
          The cost function for logistic regression uses the log-likelihood:
        </p>
        <p class="responsive-math">
          \[ J(\beta) = -\frac{1}{m}\sum_{i=1}^{m}[y_i \ln(h_\beta(x_i)) +
          (1-y_i)\ln(1-h_\beta(x_i))] \]
        </p>
      </div>

      <h4 class="accordion">
        3.3 Assumptions of logistic regression<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Logistic regression has different assumptions compared to linear
          regression:
        </p>
        <ul>
          <li>
            <strong>Linear relationship:</strong> Between the logit of the
            outcome and the independent variables
          </li>
          <li>
            <strong>Independence:</strong> Observations should be independent
          </li>
          <li>
            <strong>No multicollinearity:</strong> Independent variables should
            not be too highly correlated
          </li>
          <li>
            <strong>Large sample size:</strong> Generally requires larger sample
            sizes than linear regression
          </li>
          <li>
            <strong>No outliers:</strong> Logistic regression is sensitive to
            outliers
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        3.4 Use cases and evaluation metrics<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p><strong>Common use cases:</strong></p>
        <ul>
          <li>Email spam detection (spam vs. not spam)</li>
          <li>Medical diagnosis (disease vs. no disease)</li>
          <li>Marketing response prediction (will buy vs. won't buy)</li>
          <li>Credit approval (approve vs. reject)</li>
          <li>Image classification (cat vs. dog)</li>
        </ul>

        <p><strong>Key evaluation metrics:</strong></p>
        <ul>
          <li>
            <strong>Accuracy:</strong> \( Accuracy = \frac{TP + TN}{TP + TN + FP
            + FN} \)
          </li>
          <li>
            <strong>Precision:</strong> \( Precision = \frac{TP}{TP + FP} \)
          </li>
          <li><strong>Recall:</strong> \( Recall = \frac{TP}{TP + FN} \)</li>
          <li>
            <strong>F1-Score:</strong> \( F1 = 2 \times \frac{Precision \times
            Recall}{Precision + Recall} \)
          </li>
          <li>
            <strong>ROC-AUC:</strong> Area under the Receiver Operating
            Characteristic curve
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        3.5 Python Implementation<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Here's a complete implementation of logistic regression using
          scikit-learn:
        </p>

        <div class="responsive-code-container">
          <pre><code class="language-python">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

# Create synthetic data
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                         n_informative=2, random_state=42, n_clusters_per_class=1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit model
clf = LogisticRegression(random_state=42)
clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]

# Metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba))

# Visualization
plt.figure(figsize=(10, 4))

# Plot 1: Data points
plt.subplot(1, 2, 1)
plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='red', marker='o', label='Class 0')
plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='blue', marker='s', label='Class 1')
plt.title('Test Data Distribution')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

# Plot 2: Decision boundary
plt.subplot(1, 2, 2)
h = 0.02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)
plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='red', marker='o', edgecolors='black')
plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='blue', marker='s', edgecolors='black')
plt.title('Logistic Regression Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.tight_layout()
plt.show()
          </code></pre>
        </div>
      </div>

      <h2 id="key-differences">
        4. Key Differences Between Linear and Logistic Regression
      </h2>

      <h4 class="accordion">
        4.1 Type of output (continuous vs. categorical)<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          The most fundamental difference between linear and logistic regression
          lies in the type of output they produce:
        </p>

        <div class="responsive-table-container">
          <table border="1">
            <thead>
              <tr>
                <th>Aspect</th>
                <th>Linear Regression</th>
                <th>Logistic Regression</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Output Type</td>
                <td>Continuous numerical values</td>
                <td>Probabilities (0 to 1) or categories</td>
              </tr>
              <tr>
                <td>Range</td>
                <td>-∞ to +∞</td>
                <td>0 to 1 (for probabilities)</td>
              </tr>
              <tr>
                <td>Example Output</td>
                <td>House price: $350,000</td>
                <td>Spam probability: 0.85 or Class: Spam</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>

      <h4 class="accordion">
        4.2 Cost functions (MSE vs. log loss)<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          The choice of cost function reflects the different nature of these
          algorithms:
        </p>

        <p><strong>Linear Regression - Mean Squared Error:</strong></p>
        <p class="responsive-math">
          \[ MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2 \]
        </p>

        <p><strong>Logistic Regression - Log Loss (Cross-Entropy):</strong></p>
        <p class="responsive-math">
          \[ LogLoss = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(\hat{p}_i) +
          (1-y_i)\log(1-\hat{p}_i)] \]
        </p>

        <p>
          MSE penalizes large errors quadratically, while log loss penalizes
          confident wrong predictions more severely than uncertain wrong
          predictions.
        </p>
      </div>

      <h4 class="accordion">
        4.3 Interpretation of coefficients<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          The interpretation of coefficients differs significantly between the
          two methods:
        </p>

        <p><strong>Linear Regression:</strong></p>
        <ul>
          <li>
            Coefficients represent the change in the target variable for a
            one-unit change in the feature
          </li>
          <li>
            Example: If β₁ = 1000 in a house price model, increasing the size by
            1 sq ft increases price by $1000
          </li>
        </ul>

        <p><strong>Logistic Regression:</strong></p>
        <ul>
          <li>
            Coefficients represent the change in log-odds for a one-unit change
            in the feature
          </li>
          <li>Exponentiating coefficients gives odds ratios</li>
          <li>
            Example: If β₁ = 0.693, then exp(0.693) = 2, meaning the odds double
            for each unit increase
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        4.4 Model assumptions and limitations<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>Both methods have distinct assumptions and limitations:</p>

        <div class="responsive-table-container">
          <table border="1">
            <thead>
              <tr>
                <th>Aspect</th>
                <th>Linear Regression</th>
                <th>Logistic Regression</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Linearity</td>
                <td>Linear relationship between features and target</td>
                <td>Linear relationship between features and log-odds</td>
              </tr>
              <tr>
                <td>Error Distribution</td>
                <td>Normally distributed residuals</td>
                <td>No assumption about error distribution</td>
              </tr>
              <tr>
                <td>Variance</td>
                <td>Constant variance (homoscedasticity)</td>
                <td>No constant variance assumption</td>
              </tr>
              <tr>
                <td>Sample Size</td>
                <td>Works with smaller samples</td>
                <td>Requires larger sample sizes</td>
              </tr>
              <tr>
                <td>Outlier Sensitivity</td>
                <td>Sensitive to outliers</td>
                <td>More robust to outliers in features</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>

      <h2 id="practical-examples">5. Practical Examples</h2>

      <h4 class="accordion">
        5.1 Linear Regression Example: Predicting House Prices<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Let's implement a practical example using linear regression to predict
          house prices:
        </p>

        <div class="responsive-code-container">
          <pre><code class="language-python">
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Create synthetic house price data
np.random.seed(42)
n_samples = 1000

# Features: size (sq ft), bedrooms, age (years), location_score
size = np.random.normal(2000, 500, n_samples)
bedrooms = np.random.randint(1, 6, n_samples)
age = np.random.randint(0, 50, n_samples)
location_score = np.random.uniform(1, 10, n_samples)

# Create realistic price based on features
price = (size * 150 + bedrooms * 10000 - age * 1000 + 
         location_score * 5000 + np.random.normal(0, 20000, n_samples))

# Create DataFrame
df = pd.DataFrame({
    'size': size,
    'bedrooms': bedrooms,
    'age': age,
    'location_score': location_score,
    'price': price
})

print("Dataset shape:", df.shape)
print("\nFirst few rows:")
print(df.head())

# Prepare features and target
X = df[['size', 'bedrooms', 'age', 'location_score']]
y = df['price']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features for better interpretation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Make predictions
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# Calculate metrics
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"\nModel Performance:")
print(f"Training MSE: ${train_mse:,.2f}")
print(f"Testing MSE: ${test_mse:,.2f}")
print(f"Training R²: {train_r2:.3f}")
print(f"Testing R²: {test_r2:.3f}")

# Feature importance
feature_names = ['size', 'bedrooms', 'age', 'location_score']
coefficients = model.coef_
print(f"\nFeature Coefficients:")
for name, coef in zip(feature_names, coefficients):
    print(f"{name}: {coef:.2f}")

# Visualization
plt.figure(figsize=(12, 8))

# Plot 1: Actual vs Predicted
plt.subplot(2, 2, 1)
plt.scatter(y_test, y_test_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices')

# Plot 2: Residuals
plt.subplot(2, 2, 2)
residuals = y_test - y_test_pred
plt.scatter(y_test_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals')
plt.title('Residual Plot')

# Plot 3: Feature importance
plt.subplot(2, 2, 3)
plt.barh(feature_names, np.abs(coefficients))
plt.xlabel('Absolute Coefficient Value')
plt.title('Feature Importance')

# Plot 4: Price distribution
plt.subplot(2, 2, 4)
plt.hist(y_test, bins=30, alpha=0.7, label='Actual', density=True)
plt.hist(y_test_pred, bins=30, alpha=0.7, label='Predicted', density=True)
plt.xlabel('Price')
plt.ylabel('Density')
plt.title('Price Distribution')
plt.legend()

plt.tight_layout()
plt.show()
          </code></pre>
        </div>
      </div>

      <h4 class="accordion">
        5.2 Logistic Regression Example: Email Spam Detection<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Now let's implement a practical example using logistic regression for
          email spam detection:
        </p>

        <div class="responsive-code-container">
          <pre><code class="language-python">
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Create synthetic email data
np.random.seed(42)
n_samples = 2000

# Features for email classification
# Spam emails tend to have more capital letters, exclamation marks, certain keywords
capital_ratio = np.random.beta(2, 5, n_samples)  # Ratio of capital letters
exclamation_count = np.random.poisson(1, n_samples)  # Number of exclamation marks
word_count = np.random.normal(100, 50, n_samples)  # Total word count
suspicious_words = np.random.poisson(2, n_samples)  # Count of suspicious words
attachment_count = np.random.poisson(0.5, n_samples)  # Number of attachments

# Create spam probability based on features
spam_prob = (0.3 * capital_ratio + 0.2 * (exclamation_count / 10) + 
             0.1 * (suspicious_words / 5) + 0.2 * (attachment_count / 3) + 
             np.random.normal(0, 0.1, n_samples))

spam_prob = np.clip(spam_prob, 0, 1)  # Ensure probabilities are between 0 and 1

# Generate labels based on probability
is_spam = np.random.binomial(1, spam_prob, n_samples)

# Create DataFrame
df = pd.DataFrame({
    'capital_ratio': capital_ratio,
    'exclamation_count': exclamation_count,
    'word_count': word_count,
    'suspicious_words': suspicious_words,
    'attachment_count': attachment_count,
    'is_spam': is_spam
})

print("Dataset shape:", df.shape)
print(f"Spam ratio: {df['is_spam'].mean():.2%}")
print("\nFirst few rows:")
print(df.head())

# Prepare features and target
X = df[['capital_ratio', 'exclamation_count', 'word_count', 'suspicious_words', 'attachment_count']]
y = df['is_spam']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)
y_test_proba = model.predict_proba(X_test_scaled)[:, 1]

# Calculate metrics
accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred)
recall = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
roc_auc = roc_auc_score(y_test, y_test_proba)

print(f"\nModel Performance:")
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")
print(f"ROC-AUC: {roc_auc:.3f}")

# Feature importance (coefficients)
feature_names = ['capital_ratio', 'exclamation_count', 'word_count', 'suspicious_words', 'attachment_count']
coefficients = model.coef_[0]
print(f"\nFeature Coefficients (Log-Odds):")
for name, coef in zip(feature_names, coefficients):
    odds_ratio = np.exp(coef)
    print(f"{name}: {coef:.3f} (Odds Ratio: {odds_ratio:.3f})")

# Visualization
plt.figure(figsize=(15, 10))

# Plot 1: Confusion Matrix
plt.subplot(2, 3, 1)
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

# Plot 2: ROC Curve
from sklearn.metrics import roc_curve
plt.subplot(2, 3, 2)
fpr, tpr, _ = roc_curve(y_test, y_test_proba)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

# Plot 3: Feature importance
plt.subplot(2, 3, 3)
plt.barh(feature_names, np.abs(coefficients))
plt.xlabel('Absolute Coefficient Value')
plt.title('Feature Importance')

# Plot 4: Probability distribution
plt.subplot(2, 3, 4)
plt.hist(y_test_proba[y_test == 0], bins=30, alpha=0.7, label='Not Spam', density=True)
plt.hist(y_test_proba[y_test == 1], bins=30, alpha=0.7, label='Spam', density=True)
plt.xlabel('Predicted Probability')
plt.ylabel('Density')
plt.title('Probability Distribution')
plt.legend()

# Plot 5: Precision-Recall Curve
from sklearn.metrics import precision_recall_curve
plt.subplot(2, 3, 5)
precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_proba)
plt.plot(recall_vals, precision_vals, label=f'PR Curve (F1 = {f1:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()

# Plot 6: Feature correlation with target
plt.subplot(2, 3, 6)
correlations = df.corr()['is_spam'].drop('is_spam')
plt.barh(range(len(correlations)), correlations.values)
plt.yticks(range(len(correlations)), correlations.index)
plt.xlabel('Correlation with Spam')
plt.title('Feature Correlations')

plt.tight_layout()
plt.show()
          </code></pre>
        </div>
      </div>

      <h2 id="when-to-use">6. When to Use Which?</h2>

      <h4 class="accordion">
        6.1 Guidelines for choosing the right model<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Choosing between linear and logistic regression depends on several
          factors:
        </p>

        <p><strong>Use Linear Regression when:</strong></p>
        <ul>
          <li>
            Your target variable is continuous (e.g., price, temperature,
            weight)
          </li>
          <li>You want to predict numerical values</li>
          <li>The relationship between features and target appears linear</li>
          <li>You need to understand the magnitude of change in the target</li>
          <li>
            Examples: Sales forecasting, price prediction, risk assessment
          </li>
        </ul>

        <p><strong>Use Logistic Regression when:</strong></p>
        <ul>
          <li>Your target variable is categorical (binary or multiclass)</li>
          <li>You want to predict probabilities or class membership</li>
          <li>You need interpretable results for classification</li>
          <li>The relationship between features and log-odds is linear</li>
          <li>Examples: Medical diagnosis, spam detection, customer churn</li>
        </ul>
      </div>

      <h4 class="accordion">
        6.2 Common pitfalls<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>Avoid these common mistakes when working with regression models:</p>

        <p><strong>Linear Regression Pitfalls:</strong></p>
        <ul>
          <li>Assuming linearity without checking residual plots</li>
          <li>Ignoring multicollinearity between features</li>
          <li>Not checking for homoscedasticity (constant variance)</li>
          <li>Including irrelevant features that add noise</li>
          <li>Extrapolating beyond the range of training data</li>
        </ul>

        <p><strong>Logistic Regression Pitfalls:</strong></p>
        <ul>
          <li>Using it for continuous target variables</li>
          <li>Misinterpreting coefficients as direct effects</li>
          <li>Ignoring class imbalance in the dataset</li>
          <li>Not checking for separation in the data</li>
          <li>
            Assuming linear relationship with log-odds without verification
          </li>
        </ul>

        <p><strong>General Pitfalls:</strong></p>
        <ul>
          <li>Not scaling features when they have different units</li>
          <li>Overfitting with too many features relative to samples</li>
          <li>Not validating assumptions before applying the model</li>
          <li>Ignoring outliers that can significantly impact results</li>
        </ul>
      </div>

      <h4 class="accordion">
        6.3 Real-world considerations<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>When implementing these models in practice, consider:</p>

        <p><strong>Data Quality:</strong></p>
        <ul>
          <li>Handle missing values appropriately</li>
          <li>Check for and address outliers</li>
          <li>
            Ensure sufficient sample size, especially for logistic regression
          </li>
          <li>Validate data collection methods and potential biases</li>
        </ul>

        <p><strong>Model Interpretability:</strong></p>
        <ul>
          <li>
            Both models offer good interpretability compared to complex
            algorithms
          </li>
          <li>Linear regression coefficients are easier to interpret</li>
          <li>Logistic regression requires understanding of odds ratios</li>
          <li>Consider business stakeholder needs for explanation</li>
        </ul>

        <p><strong>Computational Efficiency:</strong></p>
        <ul>
          <li>Both models are computationally efficient</li>
          <li>Linear regression has closed-form solution</li>
          <li>Logistic regression requires iterative optimization</li>
          <li>Both scale well with large datasets</li>
        </ul>

        <p><strong>Regulatory Requirements:</strong></p>
        <ul>
          <li>Some industries require interpretable models</li>
          <li>Consider fairness and bias implications</li>
          <li>Document model assumptions and limitations</li>
          <li>Plan for model monitoring and retraining</li>
        </ul>
      </div>

      <h2 id="conclusion">7. Conclusion</h2>

      <h4 class="accordion">
        7.1 Recap of main points<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Throughout this comprehensive guide, we've explored the fundamental
          differences and applications of linear and logistic regression:
        </p>

        <p><strong>Key Takeaways:</strong></p>
        <ul>
          <li>
            <strong>Purpose:</strong> Linear regression predicts continuous
            values, while logistic regression classifies into categories
          </li>
          <li>
            <strong>Output:</strong> Linear regression produces numerical
            values, logistic regression produces probabilities
          </li>
          <li>
            <strong>Cost Functions:</strong> Linear regression uses MSE,
            logistic regression uses log-loss
          </li>
          <li>
            <strong>Assumptions:</strong> Both require linearity, but in
            different forms (direct vs. log-odds)
          </li>
          <li>
            <strong>Interpretability:</strong> Both offer excellent
            interpretability, crucial for business applications
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        7.2 Final thoughts on using these models effectively<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Linear and logistic regression remain fundamental tools in machine
          learning and statistics. Their simplicity, interpretability, and
          computational efficiency make them excellent starting points for most
          modeling projects. Here are some final recommendations:
        </p>

        <ul>
          <li>
            <strong>Start Simple:</strong> Begin with these basic models before
            moving to complex algorithms
          </li>
          <li>
            <strong>Validate Assumptions:</strong> Always check that your data
            meets the model assumptions
          </li>
          <li>
            <strong>Feature Engineering:</strong> Invest time in creating
            meaningful features
          </li>
          <li>
            <strong>Cross-Validation:</strong> Use proper validation techniques
            to assess generalization
          </li>
          <li>
            <strong>Business Context:</strong> Consider interpretability
            requirements in your domain
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        7.3 Suggestions for further reading<span
          class="arrow"
          style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          To deepen your understanding of regression techniques, consider
          exploring these advanced topics:
        </p>

        <p><strong>Advanced Regression Techniques:</strong></p>
        <ul>
          <li>
            <strong>Regularization:</strong> Ridge, Lasso, and Elastic Net
            regression
          </li>
          <li>
            <strong>Generalized Linear Models (GLMs):</strong> Extending beyond
            normal distributions
          </li>
          <li>
            <strong>Polynomial Regression:</strong> Handling non-linear
            relationships
          </li>
          <li>
            <strong>Robust Regression:</strong> Dealing with outliers and
            non-normal errors
          </li>
        </ul>

        <p><strong>Model Validation and Selection:</strong></p>
        <ul>
          <li>Cross-validation techniques and bias-variance tradeoff</li>
          <li>Information criteria (AIC, BIC) for model selection</li>
          <li>Bootstrap methods for uncertainty quantification</li>
          <li>Feature selection and dimensionality reduction</li>
        </ul>

        <p><strong>Extensions and Variations:</strong></p>
        <ul>
          <li>Multinomial logistic regression for multi-class problems</li>
          <li>Ordinal regression for ordered categorical outcomes</li>
          <li>Mixed-effects models for hierarchical data</li>
          <li>Time series regression for temporal data</li>
        </ul>
      </div>

      <h2 id="appendix">Appendix</h2>

      <h4 class="accordion">
        A.1 References<span class="arrow" style="float: left">&#9660;</span>
      </h4>
      <div>
        <ul>
          <li>
            James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013).
            <em>An Introduction to Statistical Learning</em>. Springer.
          </li>
          <li>
            Hastie, T., Tibshirani, R., & Friedman, J. (2009).
            <em>The Elements of Statistical Learning</em>. Springer.
          </li>
          <li>
            Bishop, C. M. (2006).
            <em>Pattern Recognition and Machine Learning</em>. Springer.
          </li>
          <li>
            Scikit-learn Documentation:
            <a href="https://scikit-learn.org/stable/" target="_blank"
              >https://scikit-learn.org/stable/</a
            >
          </li>
          <li>
            Hosmer Jr, D. W., Lemeshow, S., & Sturdivant, R. X. (2013).
            <em>Applied Logistic Regression</em>. John Wiley & Sons.
          </li>
        </ul>
      </div>

      <h4 class="accordion">
        A.2 Glossary of key terms<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          <strong>Coefficient:</strong> A parameter that quantifies the
          relationship between a feature and the target variable.
        </p>

        <p>
          <strong>Cross-Entropy Loss:</strong> The loss function used in
          logistic regression, also known as log-loss.
        </p>

        <p>
          <strong>Homoscedasticity:</strong> The assumption that the variance of
          residuals is constant across all levels of the independent variables.
        </p>

        <p>
          <strong>Log-Odds (Logit):</strong> The natural logarithm of the odds
          ratio, used as the link function in logistic regression.
        </p>

        <p>
          <strong>Multicollinearity:</strong> High correlation between
          independent variables that can cause problems in regression analysis.
        </p>

        <p>
          <strong>Odds Ratio:</strong> The exponential of a logistic regression
          coefficient, representing the multiplicative change in odds.
        </p>

        <p>
          <strong>Residual:</strong> The difference between observed and
          predicted values.
        </p>

        <p>
          <strong>R-squared (R²):</strong> The proportion of variance in the
          dependent variable explained by the independent variables.
        </p>

        <p>
          <strong>Sigmoid Function:</strong> The S-shaped curve used in logistic
          regression to map any real number to a value between 0 and 1.
        </p>
      </div>

      <h4 class="accordion">
        A.3 Additional Python Resources<span class="arrow" style="float: left"
          >&#9660;</span
        >
      </h4>
      <div>
        <p>
          Here are some useful Python libraries and resources for regression
          analysis:
        </p>

        <div class="responsive-code-container">
          <pre><code class="language-python">
# Essential libraries for regression analysis
import pandas as pd                    # Data manipulation
import numpy as np                     # Numerical computations
import matplotlib.pyplot as plt        # Plotting
import seaborn as sns                 # Statistical visualization

# Scikit-learn components
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

# Statistical analysis
import scipy.stats as stats          # Statistical tests
import statsmodels.api as sm         # Detailed statistical models

# Advanced regression techniques
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

# Model validation
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
          </code></pre>
        </div>

        <p><strong>Useful Online Resources:</strong></p>
        <ul>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/linear_model.html"
              target="_blank"
              >Scikit-learn Linear Models Documentation</a
            >
          </li>
          <li>
            <a
              href="https://www.statsmodels.org/stable/index.html"
              target="_blank"
              >Statsmodels Documentation</a
            >
          </li>
          <li>
            <a
              href="https://seaborn.pydata.org/tutorial/regression.html"
              target="_blank"
              >Seaborn Regression Tutorial</a
            >
          </li>
          <li>
            <a href="https://pandas.pydata.org/docs/" target="_blank"
              >Pandas Documentation</a
            >
          </li>
        </ul>
      </div>

      <div id="disqus_thread"></div>
      <script>
        var disqus_config = function () {
          this.page.url =
            "https://www.francescosannicola.com/articles/linear-logistic-regression.html";
          this.page.identifier = "linear-logistic-regression";
          this.page.title =
            "Understanding Linear and Logistic Regression: A Deep Dive and Comparison";
        };

        (function () {
          var d = document,
            s = d.createElement("script");
          s.src = "https://francescosannicola.disqus.com/embed.js";
          s.setAttribute("data-timestamp", +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
      <noscript>
        Please enable JavaScript to view the
        <a href="https://disqus.com/?ref_noscript"
          >comments powered by Disqus.</a
        >
      </noscript>
    </div>

    <button
      id="backToTopButton"
      onclick="scrollToTop()"
      style="font-size: 24px; padding: 10px 20px"
    >
      <i class="fa fa-arrow-up"></i>
    </button>

    <script>
      // Get stored dark mode state
      let isDarkMode = localStorage.getItem("darkMode") === "true";

      // Function to apply dark mode
      function applyDarkMode(dark) {
        const darkModeToggle = document.getElementById("darkModeToggle");
        const elements = [
          document.body,
          ...document.querySelectorAll(
            ".main-container, .info-container, .projects-container, .article-container, .link-container, .accordion, .code"
          ),
        ];

        // Update state
        isDarkMode = dark;
        localStorage.setItem("darkMode", dark);

        // Update UI
        elements.forEach((element) => {
          if (dark) {
            element.classList.add("dark-mode");
          } else {
            element.classList.remove("dark-mode");
          }
        });

        // Update toggle
        if (darkModeToggle) {
          darkModeToggle.checked = dark;
        }
      }

      // Initialize dark mode
      document.addEventListener("DOMContentLoaded", () => {
        const darkModeToggle = document.getElementById("darkModeToggle");

        // Apply initial state
        applyDarkMode(isDarkMode);

        // Handle toggle changes
        darkModeToggle.addEventListener("change", (e) => {
          applyDarkMode(e.target.checked);
        });
      });

      // Apply dark mode immediately if needed
      if (isDarkMode) {
        applyDarkMode(true);
      }

      var accordions = document.getElementsByClassName("accordion");

      for (var i = 0; i < accordions.length; i++) {
        var panel = accordions[i].nextElementSibling;
        panel.style.display = "none";

        accordions[i].addEventListener("click", function () {
          this.classList.toggle("active");
          var panel = this.nextElementSibling;
          if (panel.style.display === "block") {
            panel.style.display = "none";
          } else {
            panel.style.display = "block";
          }
        });
      }

      // Back to Top Button Script
      function scrollToTop() {
        document.body.scrollTop = 0; // For Safari
        document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
      }
      window.addEventListener("scroll", () => {
        const button = document.getElementById("backToTopButton");
        if (window.scrollY > 500) {
          button.style.display = "block";
        } else {
          button.style.display = "none";
        }
      });
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  </body>
</html>
